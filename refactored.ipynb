{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for b in range(1, 6):\n",
    "    D = unpickle('./cifar-10-batches-py/data_batch_%s' % b)\n",
    "    X.append( D[b'data'].reshape((-1, 3, 32, 32)).astype('uint8') )\n",
    "    Y.append( np.array(D[b'labels']))\n",
    "    names = [x.decode('utf-8') for x in D]\n",
    "\n",
    "X = np.vstack(X)\n",
    "Y = np.hstack(Y).astype('int')\n",
    "\n",
    "D = unpickle('./cifar-10-batches-py/test_batch')\n",
    "Xt = D[b'data'].reshape((-1, 3, 32, 32)).astype('uint8')\n",
    "Yt = np.array(D[b'labels']).astype('int')\n",
    "Lt = D[b'filenames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "Xa = []\n",
    "Ya = []\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    x = X[i]\n",
    "    x = torch.from_numpy(x)\n",
    "    x = to_tensor(to_img(x)).numpy()\n",
    "    \n",
    "    Xa.append(x.tolist())\n",
    "    Ya.append(Y[i].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Augmentation**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "to_img = transforms.ToPILImage()\n",
    "resized_crop = transforms.RandomResizedCrop(32)\n",
    "crop = transforms.RandomCrop([32,32])\n",
    "central_crop = transforms.CenterCrop([32,32])\n",
    "flip = transforms.RandomHorizontalFlip()\n",
    "color = transforms.ColorJitter(brightness=0.5,contrast=0,saturation=0)\n",
    "gray = transforms.Grayscale()\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create model**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        dropout_parameter = 0.3\n",
    "        \n",
    "        input_size = 3        \n",
    "        hidden_layer1_size = 48\n",
    "        hidden_layer2_size = 96\n",
    "        self.hidden_fn_layer_size = 96 * 6 * 6\n",
    "        output_size = 10\n",
    "        \n",
    "        \n",
    "        self.input_norm = nn.BatchNorm2d(input_size)\n",
    "        \n",
    "        self.layer1 = nn.Sequential( \\\n",
    "                                    nn.Conv2d(in_channels=input_size, out_channels=hidden_layer1_size, kernel_size=3, padding=1, stride=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(in_channels=hidden_layer1_size, out_channels=hidden_layer1_size, kernel_size=3, padding=0, stride=1),\n",
    "                                    nn.Dropout2d(dropout_parameter),\n",
    "                                    nn.BatchNorm2d(hidden_layer1_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(2, stride = 2)\n",
    "                                   )\n",
    "        \n",
    "        self.layer2 = nn.Sequential( \\\n",
    "                                    nn.Conv2d(in_channels=hidden_layer1_size, out_channels=hidden_layer2_size, kernel_size=3, padding=1, stride=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(in_channels=hidden_layer2_size, out_channels=hidden_layer2_size, kernel_size=3, padding=0, stride=1),\n",
    "                                    nn.Dropout2d(dropout_parameter),\n",
    "                                    nn.BatchNorm2d(hidden_layer2_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(2, stride = 2)\n",
    "                                   )\n",
    "        \n",
    "       \n",
    "        self.fn_layer = nn.Sequential( \\\n",
    "                                      nn.Linear(self.hidden_fn_layer_size, self.hidden_fn_layer_size // 2),\n",
    "                                      nn.Dropout2d(0.2),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(self.hidden_fn_layer_size // 2, output_size)\n",
    "                                     )\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(-1, self.hidden_fn_layer_size)\n",
    "        self.fn_layer(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn the model**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (input_norm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (layer1): Sequential (\n",
       "    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU ()\n",
       "    (2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): Dropout2d (p=0.3)\n",
       "    (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): ReLU ()\n",
       "    (6): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (layer2): Sequential (\n",
       "    (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU ()\n",
       "    (2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): Dropout2d (p=0.3)\n",
       "    (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): ReLU ()\n",
       "    (6): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (fn_layer): Sequential (\n",
       "    (0): Linear (3456 -> 1728)\n",
       "    (1): Dropout2d (p=0.2)\n",
       "    (2): ReLU ()\n",
       "    (3): Linear (1728 -> 10)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net().cuda()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "train_epoch_loss_list = []\n",
    "test_epoch_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/40\n",
      "Epoch loss: \n",
      "train: 391.5603790283203 \n",
      "test: 77.25887203216553\n",
      "\n",
      "\n",
      "Epoch 2/40\n",
      "Epoch loss: \n",
      "train: 364.4393630027771 \n",
      "test: 70.95874071121216\n",
      "\n",
      "\n",
      "Epoch 3/40\n",
      "Epoch loss: \n",
      "train: 352.1118640899658 \n",
      "test: 69.05934000015259\n",
      "\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_epoch = 40\n",
    "batch_size = 1000\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    train_epoch_loss = 0\n",
    "    test_epoch_loss = 0\n",
    "    \n",
    "    print ('\\nEpoch %s/%s' %(epoch+1,n_epoch))\n",
    "    Xperm = np.random.permutation(X.shape[0])\n",
    "    net.train(True)\n",
    "    for b in range(X.shape[0]//batch_size):\n",
    "        batch_idxs = Xperm[b*batch_size:(b+1)*batch_size]\n",
    "        \n",
    "        x = Variable(torch.Tensor(X[batch_idxs].tolist())).cuda()\n",
    "        y = Variable(torch.LongTensor(Y[batch_idxs].tolist())).cuda()\n",
    "        \n",
    "        y_hat = net(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += loss.data[0]\n",
    "\n",
    "    '''learn test'''\n",
    "    Xperm = np.random.permutation(Xt.shape[0])\n",
    "    net.train(False)\n",
    "    for b in range(Xt.shape[0]//batch_size):\n",
    "        batch_idxs = Xperm[b*batch_size:(b+1)*batch_size]\n",
    "        x = Variable(torch.Tensor(Xt[batch_idxs].tolist()),volatile = True).cuda()\n",
    "        y = Variable(torch.LongTensor(Yt[batch_idxs]),volatile = True).cuda()\n",
    "\n",
    "        y_hat = net(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        test_epoch_loss += loss.data[0]\n",
    "    \n",
    "    '''save loss for current epoch'''\n",
    "    train_epoch_loss_list.append(train_epoch_loss)\n",
    "    test_epoch_loss_list.append(test_epoch_loss)\n",
    "    print (\"Epoch loss: \\ntrain: %s \\ntest: %s\\n\" %(train_epoch_loss_list[-1], test_epoch_loss_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
