{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import random\n",
    "import copy\n",
    "import pandas\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classes:**\n",
    "======\n",
    "1. airplane \n",
    "2. automobile\n",
    "3. bird\n",
    "4. cat\n",
    "5. deer \n",
    "6. dog\n",
    "7. frog\n",
    "8. horse\n",
    "9. ship\n",
    "10. truck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for b in range(1, 5):\n",
    "    D = unpickle('./cifar-10-batches-py/data_batch_%s' % b)\n",
    "    X.append( D[b'data'].reshape((-1, 3, 32, 32)).astype('uint8') )\n",
    "    Y.append( np.array(D[b'labels']))\n",
    "    names = [x.decode('utf-8') for x in D]\n",
    "\n",
    "X = np.vstack(X)\n",
    "Y = np.hstack(Y).astype('int')\n",
    "\n",
    "D = unpickle('./cifar-10-batches-py/test_batch')\n",
    "Xt = D[b'data'].reshape((-1, 3, 32, 32)).astype('uint8')\n",
    "Yt = np.array(D[b'labels']).astype('int')\n",
    "Lt = D[b'filenames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some images:\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAD8CAYAAAAfZJO2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff4b3daee80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# rotate fliped along 2nd axis  k=3 times (90)\n",
    "# Augmentation!!!\n",
    "plt.imshow(np.rot90(np.flip(X[2], 2).T, k=3))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module Net\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self,batch_size = 10):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.batch_size  = batch_size\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3,64,3,padding=1) # 32x32  \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv11 = nn.Conv2d(64,64,3,padding=1) # 32x32       \n",
    "        self.bn11 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64,128,3,padding=1) # 16x16\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv22 = nn.Conv2d(128,128,3,padding=1) # 16x16\n",
    "        self.bn22 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128,64,3,padding=1) # 8x8        \n",
    "        self.bn3 = nn.BatchNorm2d(64)        \n",
    "        self.conv33 = nn.Conv2d(64,256,3,padding=1) # 8x8       \n",
    "        self.bn33 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(256,64,3,padding=1) # 4x4       \n",
    "        self.bn4 = nn.BatchNorm2d(64)       \n",
    "        self.conv44 = nn.Conv2d(64,128,3,padding=1) # 4x4       \n",
    "        self.bn44 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128,128,1,padding=0) # 2x2        \n",
    "        self.bn5 = nn.BatchNorm2d(128)       \n",
    "        self.conv55 = nn.Conv2d(128,128,1,padding=0) # 2x2       \n",
    "        self.bn55 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.lin = nn.Linear(128 * 2 * 2, batch_size)\n",
    "        \n",
    "        self.train_epoch_loss_list = []\n",
    "        self.test_epoch_loss_list = []\n",
    "        self.lr_list = []\n",
    "       \n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv11(x)\n",
    "        x = self.bn11(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.max_pool2d(x,2) # 16x16\n",
    "        \n",
    "        x = self.conv2(x) \n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv22(x) \n",
    "        x = self.bn22(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.max_pool2d(x,2) # 8x8\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv33(x)\n",
    "        x = self.bn33(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.max_pool2d(x,2) # 4x4\n",
    "        \n",
    "        x = self.conv4(x) \n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv44(x) \n",
    "        x = self.bn44(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.max_pool2d(x,2) # 2x2\n",
    "        \n",
    "        x = self.conv5(x) \n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv55(x) \n",
    "        x = self.bn55(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = x.view(-1, 128 * 2 * 2)\n",
    "        x = self.lin(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hyperparams:\n",
    "    def __init__(self):\n",
    "        self.lr0 = 0.1\n",
    "        self.epoch = 0\n",
    "        self.punch = 0.2\n",
    "        self.lr = 0.1\n",
    "        self.base = 0.1\n",
    "    \n",
    "    @property\n",
    "    def rate(self):\n",
    "        return self.epoch // 20\n",
    "    \n",
    "    def make_punch(self):\n",
    "        self.lr = self.punch\n",
    "        self.epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "net = Net(batch_size).cuda()\n",
    "hp = Hyperparams()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=hp.lr)\n",
    "\n",
    "train_epoch_loss_list = []\n",
    "test_epoch_loss_list = []\n",
    "lr_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/generic/THCStorage.cu:66",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7fafe19b37f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-n 1 -r 1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nn_epoch = 300\\n\\nfor epoch in tqdm(range(0, n_epoch)):\\n    train_epoch_loss = 0\\n    test_epoch_loss = 0\\n    \\n    #learn test\\n    Xperm = np.random.permutation(X.shape[0])\\n    net.train(True)\\n    for b in range(X.shape[0]//batch_size):\\n        batch_idxs = Xperm[b*batch_size:(b+1)*batch_size]\\n        \\n        x = Variable(torch.Tensor(X[batch_idxs].tolist())).cuda()\\n        y = Variable(torch.LongTensor(Y[batch_idxs].tolist())).cuda()\\n        \\n        y_hat = net(x)\\n        loss = loss_fn(y_hat, y)\\n        \\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n        \\n        train_epoch_loss += loss.data[0]\\n    \\n    ## learn test\\n    Xperm = np.random.permutation(Xt.shape[0])\\n    net.train(False)\\n    for b in range(Xt.shape[0]//batch_size):\\n        batch_idxs = Xperm[b*batch_size:(b+1)*batch_size]\\n        x = Variable(torch.Tensor(Xt[batch_idxs].tolist()),volatile = True).cuda()\\n        y = Variable(torch.LongTensor(Yt[batch_idxs]),volatile = True).cuda()\\n\\n        y_hat = net(x)\\n        loss = loss_fn(y_hat, y)\\n        test_epoch_loss += loss.data[0]\\n    \\n    # save loss and lr for current epoch\\n    train_epoch_loss_list.append(train_epoch_loss)\\n    test_epoch_loss_list.append(test_epoch_loss)\\n    lr_list.append(hp.lr)\\n    \\n    # update learning rate:\\n    hp.lr = hp.lr0 * ( hp.base **  hp.rate )\\n    if epoch > 2:\\n        if (abs(train_epoch_loss_list[-2] - train_epoch_loss_list[-1]) < 0.2) and (abs(train_epoch_loss_list[-2] - train_epoch_loss_list[-3]) < 0.2):\\n            hp.make_punch()\\n    \\n    for param_group in optimizer.param_groups:\\n        param_group[\\'lr\\'] = hp.lr\\n    \\n    hp.epoch += 1\\n    \\nprint (\"Epoch loss: \", train_epoch_loss_list[-1], test_epoch_loss_list[-1])\\nnet.train_epoch_loss_list = train_epoch_loss_list\\nnet.test_epoch_loss_list = test_epoch_loss_list\\nnet.lr_list = lr_list'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2101\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2103\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2104\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-61>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m   1082\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0mall_runs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0mworst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6c70ce501d63>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m         return F.batch_norm(\n\u001b[1;32m     36\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             self.training, self.momentum, self.eps)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m    637\u001b[0m                training=False, momentum=0.1, eps=1e-5):\n\u001b[1;32m    638\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/generic/THCStorage.cu:66"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "n_epoch = 300\n",
    "\n",
    "for epoch in tqdm(range(0, n_epoch)):\n",
    "    train_epoch_loss = 0\n",
    "    test_epoch_loss = 0\n",
    "    \n",
    "    #learn test\n",
    "    Xperm = np.random.permutation(X.shape[0])\n",
    "    net.train(True)\n",
    "    for b in range(X.shape[0]//batch_size):\n",
    "        batch_idxs = Xperm[b*batch_size:(b+1)*batch_size]\n",
    "        \n",
    "        x = Variable(torch.Tensor(X[batch_idxs].tolist())).cuda()\n",
    "        y = Variable(torch.LongTensor(Y[batch_idxs].tolist())).cuda()\n",
    "        \n",
    "        y_hat = net(x)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += loss.data[0]\n",
    "    \n",
    "    ## learn test\n",
    "    Xperm = np.random.permutation(Xt.shape[0])\n",
    "    net.train(False)\n",
    "    for b in range(Xt.shape[0]//batch_size):\n",
    "        batch_idxs = Xperm[b*batch_size:(b+1)*batch_size]\n",
    "        x = Variable(torch.Tensor(Xt[batch_idxs].tolist()),volatile = True).cuda()\n",
    "        y = Variable(torch.LongTensor(Yt[batch_idxs]),volatile = True).cuda()\n",
    "\n",
    "        y_hat = net(x)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        test_epoch_loss += loss.data[0]\n",
    "    \n",
    "    # save loss and lr for current epoch\n",
    "    train_epoch_loss_list.append(train_epoch_loss)\n",
    "    test_epoch_loss_list.append(test_epoch_loss)\n",
    "    lr_list.append(hp.lr)\n",
    "    \n",
    "    # update learning rate:\n",
    "    hp.lr = hp.lr0 * ( hp.base **  hp.rate )\n",
    "    if epoch > 2:\n",
    "        if (abs(train_epoch_loss_list[-2] - train_epoch_loss_list[-1]) < 0.2) and (abs(train_epoch_loss_list[-2] - train_epoch_loss_list[-3]) < 0.2):\n",
    "            hp.make_punch()\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = hp.lr\n",
    "    \n",
    "    hp.epoch += 1\n",
    "    \n",
    "print (\"Epoch loss: \", train_epoch_loss_list[-1], test_epoch_loss_list[-1])\n",
    "net.train_epoch_loss_list = train_epoch_loss_list\n",
    "net.test_epoch_loss_list = test_epoch_loss_list\n",
    "net.lr_list = lr_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_list_upd = []\n",
    "for lr in net.lr_list:\n",
    "    lr_list_upd.append(lr*100)\n",
    "    \n",
    "train_loss, = plt.plot(net.train_epoch_loss_list, 'g-',linewidth = 1, label='Train')\n",
    "test_loss, = plt.plot(net.test_epoch_loss_list, 'b-',linewidth = 1, label = \"Test\")\n",
    "lr_list, = plt.plot(lr_list_upd, 'r-',linewidth = 1, label = \"lr\")\n",
    "plt.legend(handles=[train_loss, test_loss,lr_list])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save parameters to pickle file\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net_parametrs = net.state_dict()\n",
    "with open('./result_nets.pkl','wb') as f:\n",
    "    pickle.dump(net_parametrs,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load parameters from pickle file\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./result_nets.pkl','rb') as f:\n",
    "    result_nets = pickle.load(f)\n",
    "net.load_state_dict(result_nets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn test data\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train(False)\n",
    "\n",
    "batch_size = 1000\n",
    "loss_acc = 0\n",
    "Xperm = np.random.permutation(Xt.shape[0])\n",
    "loss_fn = torch.nn.CrossEntropyLoss().cuda()\n",
    "y_hat = []\n",
    "for b in range(Xt.shape[0]//batch_size):\n",
    "    batch_idxs = Xperm[b*batch_size:(b+1)*batch_size]\n",
    "    x = Variable(torch.Tensor(Xt[batch_idxs].tolist()),volatile = True).cuda()\n",
    "    y = Variable(torch.LongTensor(Yt[batch_idxs]),volatile = True).cuda()\n",
    "    \n",
    "    \n",
    "    y_hat.append(net(x))\n",
    "    loss = loss_fn(y_hat[b], y)\n",
    "    loss_acc +=loss.data[0]\n",
    "\n",
    "print (loss_acc / (Xt.shape[0]//batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save result of test\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = pandas.DataFrame()\n",
    "d['id'] = range(len(Yt))\n",
    "res = y_hat\n",
    "if type(y_hat) == list:\n",
    "    res = y_hat[0].data.cpu().numpy()\n",
    "    for i in range(1, len(y_hat)):\n",
    "        res = np.vstack((res, y_hat[i].cpu().data.numpy()))\n",
    "        \n",
    "for i in range(10):\n",
    "    d['c%s' % i] = np.exp(res[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d.to_csv('./ground.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
