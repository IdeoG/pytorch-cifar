{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import optim\n",
    "# cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1000\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "trainloader_aug = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]), download=False),\n",
    "        batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]), download=False),\n",
    "        batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a model**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "        self.conv1 = nn.Conv2d(3, 50, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(50, 50, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.conv3 = nn.Conv2d(50, 100, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.conv4 = nn.Conv2d(100, 100, kernel_size=(3, 3), padding=(1, 1))\n",
    "        # feature map size is 8*8 by pooling\n",
    "        self.fc1 = nn.Linear(100*8*8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        \n",
    "        self.train_epoch_loss_list = []\n",
    "        self.test_epoch_loss_list = []\n",
    "        \n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.bn(x)\n",
    "        x = self.conv2(F.elu(self.conv1(x)))\n",
    "        x = F.elu(F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2)))\n",
    "        x = F.dropout(x, 0.25)\n",
    "\n",
    "        x = self.conv4(F.relu(self.conv3(x)))\n",
    "        x = F.elu(F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2)))\n",
    "        x = F.dropout(x, 0.25)\n",
    "\n",
    "        x = x.view(-1, 100*8*8)   # reshape Variable\n",
    "        x = F.dropout(F.elu(self.fc1(x)), 0.5)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn the model**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 18.1 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run(s_epoch,n_epoch,lr):\n",
    "    net.train(True)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "    for epoch in range(s_epoch,n_epoch):  \n",
    "        running_corrects= 0\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader_aug, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        print (\"Epoch %s, train accuracy %s and loss %s\" %(epoch+1,running_corrects/50000, running_loss))\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train accuracy 0.13746 and loss 114.43089604377747\n",
      "Epoch 2, train accuracy 0.22442 and loss 107.22473978996277\n",
      "Epoch 3, train accuracy 0.32148 and loss 95.03402924537659\n",
      "Epoch 4, train accuracy 0.3871 and loss 84.9089263677597\n",
      "Epoch 5, train accuracy 0.4288 and loss 78.71837139129639\n",
      "Epoch 6, train accuracy 0.45682 and loss 74.75903677940369\n",
      "Epoch 7, train accuracy 0.48514 and loss 71.31259262561798\n",
      "Epoch 8, train accuracy 0.5012 and loss 69.41212916374207\n",
      "Epoch 9, train accuracy 0.52718 and loss 66.1380227804184\n",
      "Epoch 10, train accuracy 0.54954 and loss 63.07608079910278\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(s_epoch, n_epoch, lr)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run(0,50,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, train accuracy 0.09918 and loss 519.5712230205536\n",
      "Epoch 52, train accuracy 0.09932 and loss 121.25105834007263\n",
      "Epoch 53, train accuracy 0.09916 and loss 137.70896697044373\n",
      "Epoch 54, train accuracy 0.09992 and loss 156.27223587036133\n",
      "Epoch 55, train accuracy 0.1026 and loss 116.95722818374634\n",
      "Epoch 56, train accuracy 0.1006 and loss 140.85696935653687\n",
      "Epoch 57, train accuracy 0.09964 and loss 194.7756655216217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/magics/execution.py\", line 1230, in time\n",
      "    out = eval(code, glob, local_ns)\n",
      "  File \"<timed eval>\", line 1, in <module>\n",
      "  File \"<timed exec>\", line 18, in run\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1410, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 669, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/usr/lib/python3.5/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run(50,100,0.009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(s_epoch, n_epoch, lr)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run(100,160,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eval the model**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv1): Conv2d(3, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(50, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear (6400 -> 512)\n",
       "  (fc2): Linear (512 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.8792 and loss 5.589064806699753\n",
      "\n",
      "Accuracy of plane : 75 %\n",
      "Accuracy of   car : 100 %\n",
      "Accuracy of  bird : 60 %\n",
      "Accuracy of   cat : 66 %\n",
      "Accuracy of  deer : 80 %\n",
      "Accuracy of   dog : 75 %\n",
      "Accuracy of  frog : 66 %\n",
      "Accuracy of horse : 100 %\n",
      "Accuracy of  ship : 85 %\n",
      "Accuracy of truck : 100 %\n"
     ]
    }
   ],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "running_corrects=0\n",
    "running_loss=0\n",
    "y_hat = []\n",
    "for i, data in enumerate(testloader):\n",
    "    images, labels = data\n",
    "    images, labels = images.cuda(), labels.cuda()\n",
    "    outputs = net(Variable(images, volatile=True))\n",
    "    \n",
    "    loss = criterion(outputs, Variable(labels, volatile=True))\n",
    "    running_loss += loss.data[0]\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    c = (preds == labels).squeeze()\n",
    "    running_corrects += torch.sum(preds == labels)\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1 \n",
    "    y_hat.append(outputs)\n",
    "print (\"test accuracy %s and loss %s\\n\" %(running_corrects/10000,running_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Load the result to Kaggle**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: overflow encountered in exp\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "d = pandas.DataFrame()\n",
    "d['id'] = range(10000)\n",
    "res = y_hat\n",
    "if type(y_hat) == list:\n",
    "    res = y_hat[0].data.cpu().numpy()\n",
    "    for i in range(1, len(y_hat)):\n",
    "        res = np.vstack((res, y_hat[i].cpu().data.numpy()))\n",
    "        \n",
    "for i in range(10):\n",
    "    d['c%s' % i] = np.exp(res[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.to_csv('./ground.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
