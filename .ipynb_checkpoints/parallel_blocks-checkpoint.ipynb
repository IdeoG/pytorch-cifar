{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for b in range(1, 6):\n",
    "    D = unpickle('./cifar-10-batches-py/data_batch_%s' % b)\n",
    "    X.append( D[b'data'].reshape((-1, 3, 32, 32)).astype('uint8') )\n",
    "    Y.append( np.array(D[b'labels']))\n",
    "    names = [x.decode('utf-8') for x in D]\n",
    "\n",
    "X = np.vstack(X)\n",
    "Y = np.hstack(Y).astype('int')\n",
    "\n",
    "D = unpickle('./cifar-10-batches-py/test_batch')\n",
    "Xt = D[b'data'].reshape((-1, 3, 32, 32)).astype('uint8')\n",
    "Yt = np.array(D[b'labels']).astype('int')\n",
    "Lt = D[b'filenames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "Xa = []\n",
    "Ya = []\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    x = X[i]\n",
    "    x = torch.from_numpy(x)\n",
    "    x = to_tensor(to_img(x)).numpy()\n",
    "    \n",
    "    Xa.append(x.tolist())\n",
    "    Ya.append(Y[i].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Augmentation**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "to_img = transforms.ToPILImage()\n",
    "resized_crop = transforms.RandomResizedCrop(32)\n",
    "crop = transforms.RandomCrop([32,32])\n",
    "central_crop = transforms.CenterCrop([32,32])\n",
    "flip = transforms.RandomHorizontalFlip()\n",
    "color = transforms.ColorJitter(brightness=0.5,contrast=0,saturation=0)\n",
    "gray = transforms.Grayscale()\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create model**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionA(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, pool_features):\n",
    "        super(InceptionA, self).__init__()\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 64, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, 48, kernel_size=1)\n",
    "        self.branch5x5_2 = nn.Conv2d(48, 64, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = nn.Conv2d(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = nn.Conv2d(96, 96, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = nn.Conv2d(in_channels, pool_features, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "              \n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "        \n",
    "        branch_pool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)(x)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "        \n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        dropout_parameter = 0.3\n",
    "        \n",
    "        self.hidden_fc_layer_size = 0\n",
    "        output_size = 10\n",
    "        \n",
    "        \n",
    "        self.input_norm = nn.BatchNorm2d(3)\n",
    "        \n",
    "        self.prepare_layer = nn.Sequential( \\\n",
    "                                           nn.Conv2d(3, 8, 3, padding=1),\n",
    "                                           nn.Conv2d(8, 8, 3, padding=1),\n",
    "                                           nn.Dropout2d(0.3),\n",
    "                                           nn.MaxPool2d(2, stride=2),\n",
    "                                           nn.BatchNorm2d(8),\n",
    "                                           nn.Conv2d(8, 16, 3, padding=1),\n",
    "                                           nn.Conv2d(16, 16, 3, padding=1),\n",
    "                                           nn.Dropout2d(0.3),\n",
    "                                           nn.BatchNorm2d(16),\n",
    "                                           nn.Conv2d(16, 32, 3, padding=1),\n",
    "                                           nn.Conv2d(32, 32, 3, padding=1),\n",
    "                                           nn.Dropout2d(0.3),\n",
    "                                           nn.BatchNorm2d(32),\n",
    "                                           nn.MaxPool2d(2,stride=2)\n",
    "                                          )\n",
    "        \n",
    "        self.inception1 = InceptionA(32, 32)\n",
    "        self.inception2 = InceptionA(256, 32)\n",
    "        \n",
    "        self.fc_layer = nn.Linear(1,1)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.input_norm(x)\n",
    "        x = self.prepare_layer(x)\n",
    "        x = self.inception1(x)\n",
    "        x = self.inception2(x)\n",
    "        print(x.data.size())\n",
    "        x = x.view(-1, self.hidden_fc_layer_size)\n",
    "        self.fc_layer(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn the model**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (input_norm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (prepare_layer): Sequential (\n",
       "    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): Dropout2d (p=0.3)\n",
       "    (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): Dropout2d (p=0.3)\n",
       "    (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (9): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): Dropout2d (p=0.3)\n",
       "    (12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (13): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (inception1): InceptionA (\n",
       "    (branch1x1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch5x5_1): Conv2d(32, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch5x5_2): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (branch3x3dbl_1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch3x3dbl_2): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (branch3x3dbl_3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (branch_pool): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (inception2): InceptionA (\n",
       "    (branch1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch5x5_1): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch5x5_2): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (branch3x3dbl_1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (branch3x3dbl_2): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (branch3x3dbl_3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (branch_pool): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (fc_layer): Linear (1 -> 1)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net().cuda()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "train_epoch_loss_list = []\n",
    "test_epoch_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/160 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/generic/THCStorage.cu:66",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-94ef9f9b736f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 254\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     50\u001b[0m     f = ConvNd(_pair(stride), _pair(padding), _pair(dilation), False,\n\u001b[1;32m     51\u001b[0m                _pair(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/generic/THCStorage.cu:66"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_epoch = 160\n",
    "batch_size = 1000\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    train_epoch_loss = 0\n",
    "    test_epoch_loss = 0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    print ('\\nEpoch %s/%s' %(epoch+1,n_epoch))\n",
    "    Xperm = np.random.permutation(X.shape[0])\n",
    "    net.train(True)\n",
    "    for b in range(X.shape[0]//batch_size):\n",
    "        batch_idxs = Xperm[b*batch_size:(b+1)*batch_size]\n",
    "        \n",
    "        x = Variable(torch.Tensor(X[batch_idxs].tolist())).cuda()\n",
    "        y = Variable(torch.LongTensor(Y[batch_idxs].tolist())).cuda()\n",
    "        \n",
    "        y_hat = net(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += loss.data[0]\n",
    "        _, preds = torch.max(y_hat.data, 1)\n",
    "        running_corrects += torch.sum(preds == y.data)\n",
    "    print (\"Epoch train accuracy %s\" %(running_corrects/Y.shape[0]))\n",
    "    running_corrects= 0\n",
    "    \n",
    "    '''learn test'''\n",
    "    Xperm = np.random.permutation(Xt.shape[0])\n",
    "    net.train(False)\n",
    "    for b in range(Xt.shape[0]//batch_size):\n",
    "        batch_idxs = Xperm[b*batch_size:(b+1)*batch_size]\n",
    "        x = Variable(torch.Tensor(Xt[batch_idxs].tolist()),volatile = True).cuda()\n",
    "        y = Variable(torch.LongTensor(Yt[batch_idxs]),volatile = True).cuda()\n",
    "\n",
    "        y_hat = net(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        test_epoch_loss += loss.data[0]\n",
    "        running_corrects += torch.sum(preds == y.data)\n",
    "    print (\"Epoch test accuracy %s\" %(running_corrects/Yt.shape[0]))\n",
    "    \n",
    "    '''save loss for current epoch'''\n",
    "    train_epoch_loss_list.append(train_epoch_loss)\n",
    "    test_epoch_loss_list.append(test_epoch_loss)\n",
    "    print (\"Epoch loss: \\ntrain: %s \\ntest: %s\\n\" %(train_epoch_loss_list[-1], test_epoch_loss_list[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot results**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min values and epoch\n",
      "train: 142.545029879\n",
      "test: 25.6237998009\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFXoRkRSnQi8ikuL+P22eRWzPrBwLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e6c0da080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"Min values and epoch\\ntrain: %s\\ntest: %s\" \\\n",
    "       %(np.array(train_epoch_loss_list).min(), np.array(test_epoch_loss_list).min()) )\n",
    "train_loss, = plt.plot(train_epoch_loss_list, 'g-',linewidth = 1, label='Train')\n",
    "test_loss, = plt.plot(test_epoch_loss_list, 'b-',linewidth = 1, label = \"Test\")\n",
    "plt.legend(handles=[train_loss, test_loss])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
