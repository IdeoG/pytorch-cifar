{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import optim\n",
    "# cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1000\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "trainloader_aug = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]), download=False),\n",
    "        batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "            transforms.CenterCrop(32),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]), download=False),\n",
    "        batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a model**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.feature_map_size = 4 * 4 * 200\n",
    "        \n",
    "        self.c1_layer = nn.Sequential( \\\n",
    "                                        nn.BatchNorm2d(3),\n",
    "                                        nn.Conv2d(3,50,3,padding=1),\n",
    "                                        nn.ELU(),\n",
    "                                        nn.Conv2d(50,50,3,padding=1),\n",
    "                                        nn.MaxPool2d(2,stride=2),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout2d(0.25)\n",
    "                                       )\n",
    "        self.c2_layer = nn.Sequential( \\\n",
    "                                        nn.BatchNorm2d(50),\n",
    "                                        nn.Conv2d(50,100,3,padding=1),\n",
    "                                        nn.ELU(),\n",
    "                                        nn.Conv2d(100,100,3,padding=1),\n",
    "                                        nn.MaxPool2d(2,stride=2),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout2d(0.25)\n",
    "                                       )\n",
    "        self.c3_layer = nn.Sequential( \\\n",
    "                                        nn.BatchNorm2d(100),\n",
    "                                        nn.Conv2d(100,200,3,padding=1),\n",
    "                                        nn.ELU(),\n",
    "                                        nn.Conv2d(200,200,3,padding=1),\n",
    "                                        nn.MaxPool2d(2,stride=2),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout2d(0.25)\n",
    "                                       )\n",
    "        self.fc_layer = nn.Sequential( \\\n",
    "                                        nn.Linear(self.feature_map_size, 512),\n",
    "                                        nn.ELU(),\n",
    "                                        nn.Dropout2d(),\n",
    "                                        nn.Linear(512,10)\n",
    "                                        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.c1_layer(x)\n",
    "        x = self.c2_layer(x)\n",
    "        x = self.c3_layer(x)\n",
    "        x = x.view(-1, self.feature_map_size)  \n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn the model**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.53 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''Combination of augmented data and raw data'''\n",
    "def run(s_epoch,n_epoch,lr):\n",
    "    net.train(True)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "    for epoch in range(s_epoch,n_epoch):  \n",
    "        running_corrects= 0\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader_aug, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        print (\"Epoch %s, train accuracy %s and loss %s\" %(epoch+1,running_corrects/100000, running_loss))\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train accuracy 0.35397 and loss 174.75220263004303\n",
      "Epoch 2, train accuracy 0.53339 and loss 128.60960173606873\n",
      "Epoch 3, train accuracy 0.63511 and loss 103.25240457057953\n",
      "Epoch 4, train accuracy 0.68917 and loss 88.50158601999283\n",
      "Epoch 5, train accuracy 0.72117 and loss 80.09974902868271\n",
      "Epoch 6, train accuracy 0.74442 and loss 74.01429444551468\n",
      "Epoch 7, train accuracy 0.76029 and loss 69.31868678331375\n",
      "Epoch 8, train accuracy 0.77484 and loss 64.9964998960495\n",
      "Epoch 9, train accuracy 0.78479 and loss 62.36184051632881\n",
      "Epoch 10, train accuracy 0.79598 and loss 59.24828067421913\n",
      "Epoch 11, train accuracy 0.80283 and loss 57.249764144420624\n",
      "Epoch 12, train accuracy 0.80991 and loss 55.08094772696495\n",
      "Epoch 13, train accuracy 0.81614 and loss 53.172788709402084\n",
      "Epoch 14, train accuracy 0.82288 and loss 51.13739386200905\n",
      "Epoch 15, train accuracy 0.8268 and loss 49.96586745977402\n",
      "Epoch 16, train accuracy 0.8321 and loss 48.61055809259415\n",
      "Epoch 17, train accuracy 0.8375 and loss 46.85611233115196\n",
      "Epoch 18, train accuracy 0.84172 and loss 45.930348217487335\n",
      "Epoch 19, train accuracy 0.8445 and loss 45.002009361982346\n",
      "Epoch 20, train accuracy 0.84745 and loss 44.02868935465813\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(s_epoch, n_epoch, lr)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run(0,9,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run(9,100,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eval the model**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv1): Conv2d(3, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(50, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear (6400 -> 512)\n",
       "  (fc2): Linear (512 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.8792 and loss 5.589064806699753\n",
      "\n",
      "Accuracy of plane : 75 %\n",
      "Accuracy of   car : 100 %\n",
      "Accuracy of  bird : 60 %\n",
      "Accuracy of   cat : 66 %\n",
      "Accuracy of  deer : 80 %\n",
      "Accuracy of   dog : 75 %\n",
      "Accuracy of  frog : 66 %\n",
      "Accuracy of horse : 100 %\n",
      "Accuracy of  ship : 85 %\n",
      "Accuracy of truck : 100 %\n"
     ]
    }
   ],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "running_corrects=0\n",
    "running_loss=0\n",
    "y_hat = []\n",
    "for i, data in enumerate(testloader):\n",
    "    images, labels = data\n",
    "    images, labels = images.cuda(), labels.cuda()\n",
    "    outputs = net(Variable(images, volatile=True))\n",
    "    \n",
    "    loss = criterion(outputs, Variable(labels, volatile=True))\n",
    "    running_loss += loss.data[0]\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    c = (preds == labels).squeeze()\n",
    "    running_corrects += torch.sum(preds == labels)\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1 \n",
    "    y_hat.append(outputs)\n",
    "print (\"test accuracy %s and loss %s\\n\" %(running_corrects/10000,running_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Load the result to Kaggle**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: overflow encountered in exp\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "d = pandas.DataFrame()\n",
    "d['id'] = range(10000)\n",
    "res = y_hat\n",
    "if type(y_hat) == list:\n",
    "    res = y_hat[0].data.cpu().numpy()\n",
    "    for i in range(1, len(y_hat)):\n",
    "        res = np.vstack((res, y_hat[i].cpu().data.numpy()))\n",
    "        \n",
    "for i in range(10):\n",
    "    d['c%s' % i] = np.exp(res[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.to_csv('./ground.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
