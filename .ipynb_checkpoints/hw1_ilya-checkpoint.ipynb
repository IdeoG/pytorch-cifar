{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for b in range(1, 6):\n",
    "    D = unpickle('./cifar-10-batches-py/data_batch_%s' % b)\n",
    "    X.append( D[b'data'].reshape((-1, 3, 32, 32)).astype('float'))\n",
    "    Y.append( np.array(D[b'labels']))\n",
    "X = np.vstack(X)\n",
    "print(X.shape)\n",
    "Y = np.hstack(Y).astype('int')\n",
    "\n",
    "D = unpickle('./cifar-10-batches-py/test_batch')\n",
    "Xt = D[b'data'].reshape((-1, 3, 32, 32)).astype('float')\n",
    "Yt = np.array(D[b'labels']).astype('int')\n",
    "Lt = D[b'filenames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/cifar-10-batches-py/test_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d03e7f7d9021>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/cifar-10-batches-py/test_batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# Xt = D[b'data'].reshape((-1, 3, 32, 32)).astype('float')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mYt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mb'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d03e7f7d9021>\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/cifar-10-batches-py/test_batch'"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "D = unpickle('./cifar-10-batches-py/test_batch')\n",
    "# Xt = D[b'data'].reshape((-1, 3, 32, 32)).astype('float')\n",
    "Yt = np.array(D[b'labels']).astype('int')\n",
    "Lt = D[b'filenames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.bnorm0 = torch.nn.BatchNorm2d(3)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=5) # out: 28\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.maxpool1 = torch.nn.MaxPool2d(2) # out: 14\n",
    "        self.bnorm1 = torch.nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(64, 128, kernel_size=5, padding=1) # out: 12\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # max pooling # out: 6\n",
    "        self.bnorm2 = torch.nn.BatchNorm2d(128)\n",
    "        \n",
    "        # flatten\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(128*6*6, 128*3)\n",
    "        # relu\n",
    "        # dropout\n",
    "        self.lin2 = torch.nn.Linear(128*3, 10)\n",
    "        # relu\n",
    "\n",
    "        self.softmax = torch.nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.bnorm0(x)\n",
    "         \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.bnorm1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.bnorm2(x)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = x.view(-1, 128*6*6)\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    buf = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        buf.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print( 'TRAIN Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        train_loss_curve.append([epoch, np.mean(buf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    buf = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.data[0]\n",
    "        buf.append(loss.data[0])\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('TEST Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        test_loss_curve.append([epoch, np.mean(buf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (bnorm0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv1): Conv2d(3, 36, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU ()\n",
      "  (maxpool1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (bnorm1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv2): Conv2d(36, 72, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (bnorm2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv3): Conv2d(72, 144, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (bnorm3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (lin1): Linear (576 -> 144)\n",
      "  (lin2): Linear (144 -> 10)\n",
      "  (softmax): LogSoftmax ()\n",
      ")\n",
      "\n",
      "Epoch: 0\n",
      "TRAIN Loss: 2.301 | Acc: 11.500% (92/800)\n",
      "TRAIN Loss: 2.196 | Acc: 23.387% (9542/40800)\n",
      "TEST Loss: 1.927 | Acc: 43.000% (43/100)\n",
      "TEST Loss: 1.958 | Acc: 35.020% (1786/5100)\n",
      "\n",
      "Epoch: 1\n",
      "TRAIN Loss: 1.943 | Acc: 34.000% (272/800)\n",
      "TRAIN Loss: 1.815 | Acc: 37.135% (15151/40800)\n",
      "TEST Loss: 1.534 | Acc: 51.000% (51/100)\n",
      "TEST Loss: 1.565 | Acc: 47.941% (2445/5100)\n",
      "\n",
      "Epoch: 2\n",
      "TRAIN Loss: 1.661 | Acc: 41.500% (332/800)\n",
      "TRAIN Loss: 1.562 | Acc: 44.909% (18323/40800)\n",
      "TEST Loss: 1.323 | Acc: 57.000% (57/100)\n",
      "TEST Loss: 1.357 | Acc: 54.098% (2759/5100)\n",
      "\n",
      "Epoch: 3\n",
      "TRAIN Loss: 1.439 | Acc: 51.125% (409/800)\n",
      "TRAIN Loss: 1.395 | Acc: 50.870% (20755/40800)\n",
      "TEST Loss: 1.175 | Acc: 62.000% (62/100)\n",
      "TEST Loss: 1.208 | Acc: 58.627% (2990/5100)\n",
      "\n",
      "Epoch: 4\n",
      "TRAIN Loss: 1.287 | Acc: 55.250% (442/800)\n",
      "TRAIN Loss: 1.281 | Acc: 54.926% (22410/40800)\n",
      "TEST Loss: 1.051 | Acc: 68.000% (68/100)\n",
      "TEST Loss: 1.119 | Acc: 61.216% (3122/5100)\n",
      "\n",
      "Epoch: 5\n",
      "TRAIN Loss: 1.277 | Acc: 55.125% (441/800)\n",
      "TRAIN Loss: 1.196 | Acc: 58.184% (23739/40800)\n",
      "TEST Loss: 0.984 | Acc: 68.000% (68/100)\n",
      "TEST Loss: 1.051 | Acc: 63.667% (3247/5100)\n",
      "\n",
      "Epoch: 6\n",
      "TRAIN Loss: 1.126 | Acc: 63.625% (509/800)\n",
      "TRAIN Loss: 1.134 | Acc: 60.341% (24619/40800)\n",
      "TEST Loss: 0.906 | Acc: 73.000% (73/100)\n",
      "TEST Loss: 0.989 | Acc: 65.784% (3355/5100)\n",
      "\n",
      "Epoch: 7\n",
      "TRAIN Loss: 1.113 | Acc: 61.250% (490/800)\n",
      "TRAIN Loss: 1.073 | Acc: 62.659% (25565/40800)\n",
      "TEST Loss: 0.890 | Acc: 74.000% (74/100)\n",
      "TEST Loss: 0.964 | Acc: 67.157% (3425/5100)\n",
      "\n",
      "Epoch: 8\n",
      "TRAIN Loss: 1.053 | Acc: 65.125% (521/800)\n",
      "TRAIN Loss: 1.040 | Acc: 63.936% (26086/40800)\n",
      "TEST Loss: 0.843 | Acc: 72.000% (72/100)\n",
      "TEST Loss: 0.922 | Acc: 68.529% (3495/5100)\n",
      "\n",
      "Epoch: 9\n",
      "TRAIN Loss: 0.949 | Acc: 68.375% (547/800)\n",
      "TRAIN Loss: 0.996 | Acc: 65.654% (26787/40800)\n",
      "TEST Loss: 0.869 | Acc: 73.000% (73/100)\n",
      "TEST Loss: 0.894 | Acc: 68.863% (3512/5100)\n",
      "\n",
      "Epoch: 10\n",
      "TRAIN Loss: 0.943 | Acc: 68.125% (545/800)\n",
      "TRAIN Loss: 0.966 | Acc: 66.757% (27237/40800)\n",
      "TEST Loss: 0.827 | Acc: 76.000% (76/100)\n",
      "TEST Loss: 0.879 | Acc: 69.647% (3552/5100)\n",
      "\n",
      "Epoch: 11\n",
      "TRAIN Loss: 0.911 | Acc: 67.250% (538/800)\n",
      "TRAIN Loss: 0.932 | Acc: 67.826% (27673/40800)\n",
      "TEST Loss: 0.778 | Acc: 75.000% (75/100)\n",
      "TEST Loss: 0.845 | Acc: 71.020% (3622/5100)\n",
      "\n",
      "Epoch: 12\n",
      "TRAIN Loss: 0.871 | Acc: 71.625% (573/800)\n",
      "TRAIN Loss: 0.904 | Acc: 68.865% (28097/40800)\n",
      "TEST Loss: 0.776 | Acc: 78.000% (78/100)\n",
      "TEST Loss: 0.832 | Acc: 71.529% (3648/5100)\n",
      "\n",
      "Epoch: 13\n",
      "TRAIN Loss: 0.831 | Acc: 74.250% (594/800)\n",
      "TRAIN Loss: 0.880 | Acc: 69.824% (28488/40800)\n",
      "TEST Loss: 0.749 | Acc: 77.000% (77/100)\n",
      "TEST Loss: 0.816 | Acc: 71.902% (3667/5100)\n",
      "\n",
      "Epoch: 14\n",
      "TRAIN Loss: 0.872 | Acc: 71.125% (569/800)\n",
      "TRAIN Loss: 0.852 | Acc: 70.806% (28889/40800)\n",
      "TEST Loss: 0.740 | Acc: 78.000% (78/100)\n",
      "TEST Loss: 0.797 | Acc: 72.961% (3721/5100)\n",
      "\n",
      "Epoch: 15\n",
      "TRAIN Loss: 0.854 | Acc: 72.000% (576/800)\n",
      "TRAIN Loss: 0.830 | Acc: 71.537% (29187/40800)\n",
      "TEST Loss: 0.745 | Acc: 81.000% (81/100)\n",
      "TEST Loss: 0.781 | Acc: 72.765% (3711/5100)\n",
      "\n",
      "Epoch: 16\n",
      "TRAIN Loss: 0.766 | Acc: 73.375% (587/800)\n",
      "TRAIN Loss: 0.812 | Acc: 72.027% (29387/40800)\n",
      "TEST Loss: 0.687 | Acc: 82.000% (82/100)\n",
      "TEST Loss: 0.777 | Acc: 73.510% (3749/5100)\n",
      "\n",
      "Epoch: 17\n",
      "TRAIN Loss: 0.844 | Acc: 72.250% (578/800)\n",
      "TRAIN Loss: 0.792 | Acc: 73.012% (29789/40800)\n",
      "TEST Loss: 0.722 | Acc: 77.000% (77/100)\n",
      "TEST Loss: 0.767 | Acc: 73.824% (3765/5100)\n",
      "\n",
      "Epoch: 18\n",
      "TRAIN Loss: 0.755 | Acc: 75.750% (606/800)\n",
      "TRAIN Loss: 0.772 | Acc: 73.645% (30047/40800)\n",
      "TEST Loss: 0.709 | Acc: 78.000% (78/100)\n",
      "TEST Loss: 0.749 | Acc: 74.902% (3820/5100)\n",
      "\n",
      "Epoch: 19\n",
      "TRAIN Loss: 0.792 | Acc: 71.875% (575/800)\n",
      "TRAIN Loss: 0.759 | Acc: 73.779% (30102/40800)\n",
      "TEST Loss: 0.742 | Acc: 76.000% (76/100)\n",
      "TEST Loss: 0.745 | Acc: 74.510% (3800/5100)\n",
      "\n",
      "Epoch: 20\n",
      "TRAIN Loss: 0.771 | Acc: 71.625% (573/800)\n",
      "TRAIN Loss: 0.745 | Acc: 74.414% (30361/40800)\n",
      "TEST Loss: 0.719 | Acc: 77.000% (77/100)\n",
      "TEST Loss: 0.738 | Acc: 75.196% (3835/5100)\n",
      "\n",
      "Epoch: 21\n",
      "TRAIN Loss: 0.708 | Acc: 75.250% (602/800)\n",
      "TRAIN Loss: 0.729 | Acc: 75.076% (30631/40800)\n",
      "TEST Loss: 0.724 | Acc: 76.000% (76/100)\n",
      "TEST Loss: 0.728 | Acc: 75.412% (3846/5100)\n",
      "\n",
      "Epoch: 22\n",
      "TRAIN Loss: 0.703 | Acc: 75.125% (601/800)\n",
      "TRAIN Loss: 0.713 | Acc: 75.674% (30875/40800)\n",
      "TEST Loss: 0.722 | Acc: 78.000% (78/100)\n",
      "TEST Loss: 0.721 | Acc: 75.784% (3865/5100)\n",
      "\n",
      "Epoch: 23\n",
      "TRAIN Loss: 0.719 | Acc: 76.125% (609/800)\n",
      "TRAIN Loss: 0.708 | Acc: 75.650% (30865/40800)\n",
      "TEST Loss: 0.682 | Acc: 76.000% (76/100)\n",
      "TEST Loss: 0.713 | Acc: 75.863% (3869/5100)\n",
      "\n",
      "Epoch: 24\n",
      "TRAIN Loss: 0.682 | Acc: 76.750% (614/800)\n",
      "TRAIN Loss: 0.693 | Acc: 76.355% (31153/40800)\n",
      "TEST Loss: 0.700 | Acc: 77.000% (77/100)\n",
      "TEST Loss: 0.700 | Acc: 76.451% (3899/5100)\n",
      "\n",
      "Epoch: 25\n",
      "TRAIN Loss: 0.651 | Acc: 79.250% (634/800)\n",
      "TRAIN Loss: 0.680 | Acc: 76.787% (31329/40800)\n",
      "TEST Loss: 0.640 | Acc: 81.000% (81/100)\n",
      "TEST Loss: 0.697 | Acc: 76.588% (3906/5100)\n",
      "\n",
      "Epoch: 26\n",
      "TRAIN Loss: 0.620 | Acc: 81.000% (648/800)\n",
      "TRAIN Loss: 0.666 | Acc: 77.223% (31507/40800)\n",
      "TEST Loss: 0.644 | Acc: 82.000% (82/100)\n",
      "TEST Loss: 0.689 | Acc: 77.353% (3945/5100)\n",
      "\n",
      "Epoch: 27\n",
      "TRAIN Loss: 0.617 | Acc: 78.750% (630/800)\n",
      "TRAIN Loss: 0.657 | Acc: 77.583% (31654/40800)\n",
      "TEST Loss: 0.592 | Acc: 81.000% (81/100)\n",
      "TEST Loss: 0.691 | Acc: 77.039% (3929/5100)\n",
      "\n",
      "Epoch: 28\n",
      "TRAIN Loss: 0.602 | Acc: 80.000% (640/800)\n",
      "TRAIN Loss: 0.641 | Acc: 78.115% (31871/40800)\n",
      "TEST Loss: 0.630 | Acc: 81.000% (81/100)\n",
      "TEST Loss: 0.694 | Acc: 76.608% (3907/5100)\n",
      "\n",
      "Epoch: 29\n",
      "TRAIN Loss: 0.621 | Acc: 78.125% (625/800)\n",
      "TRAIN Loss: 0.638 | Acc: 78.277% (31937/40800)\n",
      "TEST Loss: 0.629 | Acc: 83.000% (83/100)\n",
      "TEST Loss: 0.681 | Acc: 77.255% (3940/5100)\n",
      "\n",
      "Epoch: 30\n",
      "TRAIN Loss: 0.644 | Acc: 79.375% (635/800)\n",
      "TRAIN Loss: 0.627 | Acc: 78.596% (32067/40800)\n",
      "TEST Loss: 0.586 | Acc: 83.000% (83/100)\n",
      "TEST Loss: 0.678 | Acc: 77.843% (3970/5100)\n",
      "\n",
      "Epoch: 31\n",
      "TRAIN Loss: 0.642 | Acc: 76.625% (613/800)\n",
      "TRAIN Loss: 0.623 | Acc: 78.711% (32114/40800)\n",
      "TEST Loss: 0.601 | Acc: 83.000% (83/100)\n",
      "TEST Loss: 0.670 | Acc: 78.098% (3983/5100)\n",
      "\n",
      "Epoch: 32\n",
      "TRAIN Loss: 0.638 | Acc: 77.500% (620/800)\n",
      "TRAIN Loss: 0.607 | Acc: 79.311% (32359/40800)\n",
      "TEST Loss: 0.618 | Acc: 82.000% (82/100)\n",
      "TEST Loss: 0.682 | Acc: 77.824% (3969/5100)\n",
      "\n",
      "Epoch: 33\n",
      "TRAIN Loss: 0.631 | Acc: 77.125% (617/800)\n",
      "TRAIN Loss: 0.599 | Acc: 79.610% (32481/40800)\n",
      "TEST Loss: 0.599 | Acc: 81.000% (81/100)\n",
      "TEST Loss: 0.665 | Acc: 77.843% (3970/5100)\n",
      "\n",
      "Epoch: 34\n",
      "TRAIN Loss: 0.600 | Acc: 79.750% (638/800)\n",
      "TRAIN Loss: 0.594 | Acc: 79.775% (32548/40800)\n",
      "TEST Loss: 0.675 | Acc: 82.000% (82/100)\n",
      "TEST Loss: 0.667 | Acc: 78.235% (3990/5100)\n",
      "\n",
      "Epoch: 35\n",
      "TRAIN Loss: 0.572 | Acc: 80.500% (644/800)\n",
      "TRAIN Loss: 0.589 | Acc: 79.792% (32555/40800)\n",
      "TEST Loss: 0.595 | Acc: 83.000% (83/100)\n",
      "TEST Loss: 0.655 | Acc: 78.569% (4007/5100)\n",
      "\n",
      "Epoch: 36\n",
      "TRAIN Loss: 0.584 | Acc: 79.500% (636/800)\n",
      "TRAIN Loss: 0.580 | Acc: 80.255% (32744/40800)\n",
      "TEST Loss: 0.573 | Acc: 81.000% (81/100)\n",
      "TEST Loss: 0.656 | Acc: 78.510% (4004/5100)\n",
      "\n",
      "Epoch: 37\n",
      "TRAIN Loss: 0.588 | Acc: 79.125% (633/800)\n",
      "TRAIN Loss: 0.572 | Acc: 80.370% (32791/40800)\n",
      "TEST Loss: 0.595 | Acc: 84.000% (84/100)\n",
      "TEST Loss: 0.644 | Acc: 78.765% (4017/5100)\n",
      "\n",
      "Epoch: 38\n",
      "TRAIN Loss: 0.511 | Acc: 82.125% (657/800)\n",
      "TRAIN Loss: 0.565 | Acc: 80.738% (32941/40800)\n",
      "TEST Loss: 0.586 | Acc: 83.000% (83/100)\n",
      "TEST Loss: 0.647 | Acc: 78.922% (4025/5100)\n",
      "\n",
      "Epoch: 39\n",
      "TRAIN Loss: 0.513 | Acc: 82.875% (663/800)\n",
      "TRAIN Loss: 0.556 | Acc: 80.806% (32969/40800)\n",
      "TEST Loss: 0.548 | Acc: 84.000% (84/100)\n",
      "TEST Loss: 0.633 | Acc: 79.059% (4032/5100)\n",
      "\n",
      "Epoch: 40\n",
      "TRAIN Loss: 0.504 | Acc: 82.000% (656/800)\n",
      "TRAIN Loss: 0.552 | Acc: 81.221% (33138/40800)\n",
      "TEST Loss: 0.554 | Acc: 84.000% (84/100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Loss: 0.639 | Acc: 78.902% (4024/5100)\n",
      "\n",
      "Epoch: 41\n",
      "TRAIN Loss: 0.534 | Acc: 82.250% (658/800)\n",
      "TRAIN Loss: 0.546 | Acc: 81.414% (33217/40800)\n",
      "TEST Loss: 0.531 | Acc: 84.000% (84/100)\n",
      "TEST Loss: 0.643 | Acc: 78.686% (4013/5100)\n",
      "\n",
      "Epoch: 42\n",
      "TRAIN Loss: 0.551 | Acc: 81.250% (650/800)\n",
      "TRAIN Loss: 0.541 | Acc: 81.348% (33190/40800)\n",
      "TEST Loss: 0.539 | Acc: 85.000% (85/100)\n",
      "TEST Loss: 0.644 | Acc: 78.922% (4025/5100)\n",
      "\n",
      "Epoch: 43\n",
      "TRAIN Loss: 0.553 | Acc: 81.125% (649/800)\n",
      "TRAIN Loss: 0.537 | Acc: 81.843% (33392/40800)\n",
      "TEST Loss: 0.539 | Acc: 83.000% (83/100)\n",
      "TEST Loss: 0.644 | Acc: 79.235% (4041/5100)\n",
      "\n",
      "Epoch: 44\n",
      "TRAIN Loss: 0.494 | Acc: 83.250% (666/800)\n",
      "TRAIN Loss: 0.529 | Acc: 82.012% (33461/40800)\n",
      "TEST Loss: 0.550 | Acc: 83.000% (83/100)\n",
      "TEST Loss: 0.651 | Acc: 78.922% (4025/5100)\n",
      "\n",
      "Epoch: 45\n",
      "TRAIN Loss: 0.498 | Acc: 84.250% (674/800)\n",
      "TRAIN Loss: 0.524 | Acc: 82.061% (33481/40800)\n",
      "TEST Loss: 0.494 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.634 | Acc: 79.314% (4045/5100)\n",
      "\n",
      "Epoch: 46\n",
      "TRAIN Loss: 0.532 | Acc: 83.250% (666/800)\n",
      "TRAIN Loss: 0.512 | Acc: 82.652% (33722/40800)\n",
      "TEST Loss: 0.548 | Acc: 84.000% (84/100)\n",
      "TEST Loss: 0.649 | Acc: 79.039% (4031/5100)\n",
      "\n",
      "Epoch: 47\n",
      "TRAIN Loss: 0.509 | Acc: 82.500% (660/800)\n",
      "TRAIN Loss: 0.510 | Acc: 82.725% (33752/40800)\n",
      "TEST Loss: 0.497 | Acc: 85.000% (85/100)\n",
      "TEST Loss: 0.631 | Acc: 78.961% (4027/5100)\n",
      "\n",
      "Epoch: 48\n",
      "TRAIN Loss: 0.459 | Acc: 83.875% (671/800)\n",
      "TRAIN Loss: 0.505 | Acc: 82.789% (33778/40800)\n",
      "TEST Loss: 0.562 | Acc: 81.000% (81/100)\n",
      "TEST Loss: 0.628 | Acc: 79.314% (4045/5100)\n",
      "\n",
      "Epoch: 49\n",
      "TRAIN Loss: 0.433 | Acc: 86.000% (688/800)\n",
      "TRAIN Loss: 0.503 | Acc: 82.833% (33796/40800)\n",
      "TEST Loss: 0.521 | Acc: 83.000% (83/100)\n",
      "TEST Loss: 0.632 | Acc: 79.706% (4065/5100)\n",
      "\n",
      "Epoch: 50\n",
      "TRAIN Loss: 0.540 | Acc: 81.875% (655/800)\n",
      "TRAIN Loss: 0.496 | Acc: 83.047% (33883/40800)\n",
      "TEST Loss: 0.508 | Acc: 84.000% (84/100)\n",
      "TEST Loss: 0.632 | Acc: 80.255% (4093/5100)\n",
      "\n",
      "Epoch: 51\n",
      "TRAIN Loss: 0.514 | Acc: 82.625% (661/800)\n",
      "TRAIN Loss: 0.494 | Acc: 83.096% (33903/40800)\n",
      "TEST Loss: 0.477 | Acc: 84.000% (84/100)\n",
      "TEST Loss: 0.621 | Acc: 80.137% (4087/5100)\n",
      "\n",
      "Epoch: 52\n",
      "TRAIN Loss: 0.530 | Acc: 81.250% (650/800)\n",
      "TRAIN Loss: 0.490 | Acc: 83.306% (33989/40800)\n",
      "TEST Loss: 0.505 | Acc: 84.000% (84/100)\n",
      "TEST Loss: 0.622 | Acc: 79.961% (4078/5100)\n",
      "\n",
      "Epoch: 53\n",
      "TRAIN Loss: 0.486 | Acc: 84.625% (677/800)\n",
      "TRAIN Loss: 0.484 | Acc: 83.282% (33979/40800)\n",
      "TEST Loss: 0.512 | Acc: 84.000% (84/100)\n",
      "TEST Loss: 0.625 | Acc: 79.843% (4072/5100)\n",
      "\n",
      "Epoch: 54\n",
      "TRAIN Loss: 0.437 | Acc: 85.750% (686/800)\n",
      "TRAIN Loss: 0.485 | Acc: 83.426% (34038/40800)\n",
      "TEST Loss: 0.520 | Acc: 82.000% (82/100)\n",
      "TEST Loss: 0.638 | Acc: 80.020% (4081/5100)\n",
      "\n",
      "Epoch: 55\n",
      "TRAIN Loss: 0.479 | Acc: 84.000% (672/800)\n",
      "TRAIN Loss: 0.474 | Acc: 83.946% (34250/40800)\n",
      "TEST Loss: 0.491 | Acc: 85.000% (85/100)\n",
      "TEST Loss: 0.627 | Acc: 79.980% (4079/5100)\n",
      "\n",
      "Epoch: 56\n",
      "TRAIN Loss: 0.483 | Acc: 83.125% (665/800)\n",
      "TRAIN Loss: 0.469 | Acc: 84.020% (34280/40800)\n",
      "TEST Loss: 0.481 | Acc: 83.000% (83/100)\n",
      "TEST Loss: 0.630 | Acc: 79.667% (4063/5100)\n",
      "\n",
      "Epoch: 57\n",
      "TRAIN Loss: 0.459 | Acc: 84.750% (678/800)\n",
      "TRAIN Loss: 0.466 | Acc: 84.142% (34330/40800)\n",
      "TEST Loss: 0.479 | Acc: 82.000% (82/100)\n",
      "TEST Loss: 0.643 | Acc: 79.725% (4066/5100)\n",
      "\n",
      "Epoch: 58\n",
      "TRAIN Loss: 0.473 | Acc: 83.500% (668/800)\n",
      "TRAIN Loss: 0.457 | Acc: 84.306% (34397/40800)\n",
      "TEST Loss: 0.490 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.620 | Acc: 80.176% (4089/5100)\n",
      "\n",
      "Epoch: 59\n",
      "TRAIN Loss: 0.453 | Acc: 85.125% (681/800)\n",
      "TRAIN Loss: 0.454 | Acc: 84.485% (34470/40800)\n",
      "TEST Loss: 0.506 | Acc: 85.000% (85/100)\n",
      "TEST Loss: 0.626 | Acc: 79.843% (4072/5100)\n",
      "\n",
      "Epoch: 60\n",
      "TRAIN Loss: 0.505 | Acc: 82.625% (661/800)\n",
      "TRAIN Loss: 0.450 | Acc: 84.618% (34524/40800)\n",
      "TEST Loss: 0.480 | Acc: 86.000% (86/100)\n",
      "TEST Loss: 0.625 | Acc: 80.412% (4101/5100)\n",
      "\n",
      "Epoch: 61\n",
      "TRAIN Loss: 0.384 | Acc: 88.375% (707/800)\n",
      "TRAIN Loss: 0.446 | Acc: 84.904% (34641/40800)\n",
      "TEST Loss: 0.475 | Acc: 85.000% (85/100)\n",
      "TEST Loss: 0.631 | Acc: 79.882% (4074/5100)\n",
      "\n",
      "Epoch: 62\n",
      "TRAIN Loss: 0.441 | Acc: 84.875% (679/800)\n",
      "TRAIN Loss: 0.441 | Acc: 85.098% (34720/40800)\n",
      "TEST Loss: 0.493 | Acc: 84.000% (84/100)\n",
      "TEST Loss: 0.617 | Acc: 80.294% (4095/5100)\n",
      "\n",
      "Epoch: 63\n",
      "TRAIN Loss: 0.483 | Acc: 84.625% (677/800)\n",
      "TRAIN Loss: 0.439 | Acc: 85.152% (34742/40800)\n",
      "TEST Loss: 0.464 | Acc: 85.000% (85/100)\n",
      "TEST Loss: 0.631 | Acc: 80.235% (4092/5100)\n",
      "\n",
      "Epoch: 64\n",
      "TRAIN Loss: 0.391 | Acc: 85.500% (684/800)\n",
      "TRAIN Loss: 0.442 | Acc: 84.801% (34599/40800)\n",
      "TEST Loss: 0.409 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.620 | Acc: 80.490% (4105/5100)\n",
      "\n",
      "Epoch: 65\n",
      "TRAIN Loss: 0.437 | Acc: 86.625% (693/800)\n",
      "TRAIN Loss: 0.437 | Acc: 84.873% (34628/40800)\n",
      "TEST Loss: 0.449 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.620 | Acc: 80.902% (4126/5100)\n",
      "\n",
      "Epoch: 66\n",
      "TRAIN Loss: 0.428 | Acc: 84.875% (679/800)\n",
      "TRAIN Loss: 0.434 | Acc: 85.196% (34760/40800)\n",
      "TEST Loss: 0.452 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.628 | Acc: 80.157% (4088/5100)\n",
      "\n",
      "Epoch: 67\n",
      "TRAIN Loss: 0.454 | Acc: 85.250% (682/800)\n",
      "TRAIN Loss: 0.430 | Acc: 85.373% (34832/40800)\n",
      "TEST Loss: 0.477 | Acc: 84.000% (84/100)\n",
      "TEST Loss: 0.614 | Acc: 80.588% (4110/5100)\n",
      "\n",
      "Epoch: 68\n",
      "TRAIN Loss: 0.414 | Acc: 86.250% (690/800)\n",
      "TRAIN Loss: 0.428 | Acc: 85.375% (34833/40800)\n",
      "TEST Loss: 0.446 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.618 | Acc: 80.608% (4111/5100)\n",
      "\n",
      "Epoch: 69\n",
      "TRAIN Loss: 0.421 | Acc: 85.250% (682/800)\n",
      "TRAIN Loss: 0.422 | Acc: 85.708% (34969/40800)\n",
      "TEST Loss: 0.491 | Acc: 83.000% (83/100)\n",
      "TEST Loss: 0.616 | Acc: 80.569% (4109/5100)\n",
      "\n",
      "Epoch: 70\n",
      "TRAIN Loss: 0.455 | Acc: 85.000% (680/800)\n",
      "TRAIN Loss: 0.416 | Acc: 85.804% (35008/40800)\n",
      "TEST Loss: 0.470 | Acc: 86.000% (86/100)\n",
      "TEST Loss: 0.609 | Acc: 80.863% (4124/5100)\n",
      "\n",
      "Epoch: 71\n",
      "TRAIN Loss: 0.388 | Acc: 87.750% (702/800)\n",
      "TRAIN Loss: 0.414 | Acc: 85.863% (35032/40800)\n",
      "TEST Loss: 0.473 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.614 | Acc: 80.745% (4118/5100)\n",
      "\n",
      "Epoch: 72\n",
      "TRAIN Loss: 0.398 | Acc: 86.875% (695/800)\n",
      "TRAIN Loss: 0.409 | Acc: 85.966% (35074/40800)\n",
      "TEST Loss: 0.441 | Acc: 86.000% (86/100)\n",
      "TEST Loss: 0.607 | Acc: 81.000% (4131/5100)\n",
      "\n",
      "Epoch: 73\n",
      "TRAIN Loss: 0.415 | Acc: 85.875% (687/800)\n",
      "TRAIN Loss: 0.412 | Acc: 85.877% (35038/40800)\n",
      "TEST Loss: 0.455 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.623 | Acc: 80.373% (4099/5100)\n",
      "\n",
      "Epoch: 74\n",
      "TRAIN Loss: 0.373 | Acc: 85.750% (686/800)\n",
      "TRAIN Loss: 0.410 | Acc: 85.936% (35062/40800)\n",
      "TEST Loss: 0.474 | Acc: 85.000% (85/100)\n",
      "TEST Loss: 0.617 | Acc: 80.569% (4109/5100)\n",
      "\n",
      "Epoch: 75\n",
      "TRAIN Loss: 0.416 | Acc: 86.375% (691/800)\n",
      "TRAIN Loss: 0.387 | Acc: 86.740% (35390/40800)\n",
      "TEST Loss: 0.455 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.607 | Acc: 80.941% (4128/5100)\n",
      "\n",
      "Epoch: 76\n",
      "TRAIN Loss: 0.324 | Acc: 87.750% (702/800)\n",
      "TRAIN Loss: 0.383 | Acc: 86.858% (35438/40800)\n",
      "TEST Loss: 0.456 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.609 | Acc: 81.059% (4134/5100)\n",
      "\n",
      "Epoch: 77\n",
      "TRAIN Loss: 0.375 | Acc: 87.000% (696/800)\n",
      "TRAIN Loss: 0.388 | Acc: 86.701% (35374/40800)\n",
      "TEST Loss: 0.454 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.608 | Acc: 81.059% (4134/5100)\n",
      "\n",
      "Epoch: 78\n",
      "TRAIN Loss: 0.347 | Acc: 88.875% (711/800)\n",
      "TRAIN Loss: 0.382 | Acc: 86.922% (35464/40800)\n",
      "TEST Loss: 0.444 | Acc: 86.000% (86/100)\n",
      "TEST Loss: 0.606 | Acc: 81.039% (4133/5100)\n",
      "\n",
      "Epoch: 79\n",
      "TRAIN Loss: 0.362 | Acc: 88.625% (709/800)\n",
      "TRAIN Loss: 0.372 | Acc: 87.311% (35623/40800)\n",
      "TEST Loss: 0.450 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.605 | Acc: 81.118% (4137/5100)\n",
      "\n",
      "Epoch: 80\n",
      "TRAIN Loss: 0.380 | Acc: 87.000% (696/800)\n",
      "TRAIN Loss: 0.379 | Acc: 87.091% (35533/40800)\n",
      "TEST Loss: 0.443 | Acc: 86.000% (86/100)\n",
      "TEST Loss: 0.606 | Acc: 81.118% (4137/5100)\n",
      "\n",
      "Epoch: 81\n",
      "TRAIN Loss: 0.409 | Acc: 85.750% (686/800)\n",
      "TRAIN Loss: 0.386 | Acc: 86.713% (35379/40800)\n",
      "TEST Loss: 0.443 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.607 | Acc: 81.118% (4137/5100)\n",
      "\n",
      "Epoch: 82\n",
      "TRAIN Loss: 0.418 | Acc: 86.500% (692/800)\n",
      "TRAIN Loss: 0.379 | Acc: 87.147% (35556/40800)\n",
      "TEST Loss: 0.440 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.604 | Acc: 81.137% (4138/5100)\n",
      "\n",
      "Epoch: 83\n",
      "TRAIN Loss: 0.401 | Acc: 85.875% (687/800)\n",
      "TRAIN Loss: 0.376 | Acc: 87.213% (35583/40800)\n",
      "TEST Loss: 0.438 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.604 | Acc: 81.353% (4149/5100)\n",
      "\n",
      "Epoch: 84\n",
      "TRAIN Loss: 0.353 | Acc: 87.875% (703/800)\n",
      "TRAIN Loss: 0.376 | Acc: 87.142% (35554/40800)\n",
      "TEST Loss: 0.441 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.606 | Acc: 81.314% (4147/5100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 85\n",
      "TRAIN Loss: 0.404 | Acc: 85.750% (686/800)\n",
      "TRAIN Loss: 0.381 | Acc: 86.897% (35454/40800)\n",
      "TEST Loss: 0.434 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.603 | Acc: 81.255% (4144/5100)\n",
      "\n",
      "Epoch: 86\n",
      "TRAIN Loss: 0.361 | Acc: 86.875% (695/800)\n",
      "TRAIN Loss: 0.375 | Acc: 87.049% (35516/40800)\n",
      "TEST Loss: 0.430 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.606 | Acc: 81.275% (4145/5100)\n",
      "\n",
      "Epoch: 87\n",
      "TRAIN Loss: 0.367 | Acc: 86.875% (695/800)\n",
      "TRAIN Loss: 0.373 | Acc: 87.279% (35610/40800)\n",
      "TEST Loss: 0.425 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.606 | Acc: 81.196% (4141/5100)\n",
      "\n",
      "Epoch: 88\n",
      "TRAIN Loss: 0.399 | Acc: 86.875% (695/800)\n",
      "TRAIN Loss: 0.376 | Acc: 87.203% (35579/40800)\n",
      "TEST Loss: 0.444 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.607 | Acc: 81.373% (4150/5100)\n",
      "\n",
      "Epoch: 89\n",
      "TRAIN Loss: 0.413 | Acc: 85.500% (684/800)\n",
      "TRAIN Loss: 0.375 | Acc: 87.167% (35564/40800)\n",
      "TEST Loss: 0.439 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.609 | Acc: 81.098% (4136/5100)\n",
      "\n",
      "Epoch: 90\n",
      "TRAIN Loss: 0.379 | Acc: 88.125% (705/800)\n",
      "TRAIN Loss: 0.373 | Acc: 87.201% (35578/40800)\n",
      "TEST Loss: 0.436 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.606 | Acc: 81.294% (4146/5100)\n",
      "\n",
      "Epoch: 91\n",
      "TRAIN Loss: 0.349 | Acc: 89.125% (713/800)\n",
      "TRAIN Loss: 0.372 | Acc: 87.208% (35581/40800)\n",
      "TEST Loss: 0.433 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.609 | Acc: 81.157% (4139/5100)\n",
      "\n",
      "Epoch: 92\n",
      "TRAIN Loss: 0.353 | Acc: 87.625% (701/800)\n",
      "TRAIN Loss: 0.373 | Acc: 87.277% (35609/40800)\n",
      "TEST Loss: 0.437 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.611 | Acc: 81.314% (4147/5100)\n",
      "\n",
      "Epoch: 93\n",
      "TRAIN Loss: 0.405 | Acc: 87.125% (697/800)\n",
      "TRAIN Loss: 0.369 | Acc: 87.321% (35627/40800)\n",
      "TEST Loss: 0.427 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.609 | Acc: 81.157% (4139/5100)\n",
      "\n",
      "Epoch: 94\n",
      "TRAIN Loss: 0.383 | Acc: 87.125% (697/800)\n",
      "TRAIN Loss: 0.375 | Acc: 87.176% (35568/40800)\n",
      "TEST Loss: 0.435 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.611 | Acc: 81.078% (4135/5100)\n",
      "\n",
      "Epoch: 95\n",
      "TRAIN Loss: 0.378 | Acc: 86.625% (693/800)\n",
      "TRAIN Loss: 0.372 | Acc: 87.377% (35650/40800)\n",
      "TEST Loss: 0.436 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.611 | Acc: 81.255% (4144/5100)\n",
      "\n",
      "Epoch: 96\n",
      "TRAIN Loss: 0.367 | Acc: 87.875% (703/800)\n",
      "TRAIN Loss: 0.375 | Acc: 87.328% (35630/40800)\n",
      "TEST Loss: 0.429 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.611 | Acc: 81.275% (4145/5100)\n",
      "\n",
      "Epoch: 97\n",
      "TRAIN Loss: 0.420 | Acc: 85.625% (685/800)\n",
      "TRAIN Loss: 0.369 | Acc: 87.277% (35609/40800)\n",
      "TEST Loss: 0.437 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.610 | Acc: 81.078% (4135/5100)\n",
      "\n",
      "Epoch: 98\n",
      "TRAIN Loss: 0.399 | Acc: 86.750% (694/800)\n",
      "TRAIN Loss: 0.369 | Acc: 87.461% (35684/40800)\n",
      "TEST Loss: 0.433 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.613 | Acc: 81.157% (4139/5100)\n",
      "\n",
      "Epoch: 99\n",
      "TRAIN Loss: 0.327 | Acc: 88.500% (708/800)\n",
      "TRAIN Loss: 0.370 | Acc: 87.319% (35626/40800)\n",
      "TEST Loss: 0.429 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.613 | Acc: 81.020% (4132/5100)\n",
      "\n",
      "Epoch: 100\n",
      "TRAIN Loss: 0.407 | Acc: 86.750% (694/800)\n",
      "TRAIN Loss: 0.362 | Acc: 87.574% (35730/40800)\n",
      "TEST Loss: 0.424 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.610 | Acc: 81.255% (4144/5100)\n",
      "\n",
      "Epoch: 101\n",
      "TRAIN Loss: 0.379 | Acc: 88.125% (705/800)\n",
      "TRAIN Loss: 0.372 | Acc: 87.380% (35651/40800)\n",
      "TEST Loss: 0.437 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.612 | Acc: 81.157% (4139/5100)\n",
      "\n",
      "Epoch: 102\n",
      "TRAIN Loss: 0.360 | Acc: 87.875% (703/800)\n",
      "TRAIN Loss: 0.369 | Acc: 87.375% (35649/40800)\n",
      "TEST Loss: 0.436 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.610 | Acc: 81.294% (4146/5100)\n",
      "\n",
      "Epoch: 103\n",
      "TRAIN Loss: 0.392 | Acc: 86.750% (694/800)\n",
      "TRAIN Loss: 0.364 | Acc: 87.581% (35733/40800)\n",
      "TEST Loss: 0.435 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.610 | Acc: 81.392% (4151/5100)\n",
      "\n",
      "Epoch: 104\n",
      "TRAIN Loss: 0.393 | Acc: 86.750% (694/800)\n",
      "TRAIN Loss: 0.368 | Acc: 87.522% (35709/40800)\n",
      "TEST Loss: 0.445 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.609 | Acc: 81.294% (4146/5100)\n",
      "\n",
      "Epoch: 105\n",
      "TRAIN Loss: 0.392 | Acc: 85.500% (684/800)\n",
      "TRAIN Loss: 0.368 | Acc: 87.216% (35584/40800)\n",
      "TEST Loss: 0.440 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.610 | Acc: 81.118% (4137/5100)\n",
      "\n",
      "Epoch: 106\n",
      "TRAIN Loss: 0.366 | Acc: 86.625% (693/800)\n",
      "TRAIN Loss: 0.365 | Acc: 87.520% (35708/40800)\n",
      "TEST Loss: 0.429 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.612 | Acc: 81.137% (4138/5100)\n",
      "\n",
      "Epoch: 107\n",
      "TRAIN Loss: 0.359 | Acc: 86.875% (695/800)\n",
      "TRAIN Loss: 0.368 | Acc: 87.304% (35620/40800)\n",
      "TEST Loss: 0.427 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.610 | Acc: 81.176% (4140/5100)\n",
      "\n",
      "Epoch: 108\n",
      "TRAIN Loss: 0.347 | Acc: 89.000% (712/800)\n",
      "TRAIN Loss: 0.363 | Acc: 87.520% (35708/40800)\n",
      "TEST Loss: 0.427 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.613 | Acc: 81.314% (4147/5100)\n",
      "\n",
      "Epoch: 109\n",
      "TRAIN Loss: 0.325 | Acc: 87.750% (702/800)\n",
      "TRAIN Loss: 0.367 | Acc: 87.485% (35694/40800)\n",
      "TEST Loss: 0.437 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.610 | Acc: 81.137% (4138/5100)\n",
      "\n",
      "Epoch: 110\n",
      "TRAIN Loss: 0.364 | Acc: 87.000% (696/800)\n",
      "TRAIN Loss: 0.363 | Acc: 87.637% (35756/40800)\n",
      "TEST Loss: 0.440 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.612 | Acc: 81.059% (4134/5100)\n",
      "\n",
      "Epoch: 111\n",
      "TRAIN Loss: 0.351 | Acc: 87.375% (699/800)\n",
      "TRAIN Loss: 0.363 | Acc: 87.657% (35764/40800)\n",
      "TEST Loss: 0.436 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.611 | Acc: 81.137% (4138/5100)\n",
      "\n",
      "Epoch: 112\n",
      "TRAIN Loss: 0.378 | Acc: 86.625% (693/800)\n",
      "TRAIN Loss: 0.364 | Acc: 87.789% (35818/40800)\n",
      "TEST Loss: 0.435 | Acc: 86.000% (86/100)\n",
      "TEST Loss: 0.610 | Acc: 81.176% (4140/5100)\n",
      "\n",
      "Epoch: 113\n",
      "TRAIN Loss: 0.303 | Acc: 89.625% (717/800)\n",
      "TRAIN Loss: 0.362 | Acc: 87.596% (35739/40800)\n",
      "TEST Loss: 0.438 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.611 | Acc: 81.255% (4144/5100)\n",
      "\n",
      "Epoch: 114\n",
      "TRAIN Loss: 0.360 | Acc: 87.500% (700/800)\n",
      "TRAIN Loss: 0.360 | Acc: 87.473% (35689/40800)\n",
      "TEST Loss: 0.436 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.611 | Acc: 81.235% (4143/5100)\n",
      "\n",
      "Epoch: 115\n",
      "TRAIN Loss: 0.316 | Acc: 88.875% (711/800)\n",
      "TRAIN Loss: 0.362 | Acc: 87.674% (35771/40800)\n",
      "TEST Loss: 0.436 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.611 | Acc: 81.176% (4140/5100)\n",
      "\n",
      "Epoch: 116\n",
      "TRAIN Loss: 0.330 | Acc: 89.250% (714/800)\n",
      "TRAIN Loss: 0.359 | Acc: 87.728% (35793/40800)\n",
      "TEST Loss: 0.439 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.612 | Acc: 80.961% (4129/5100)\n",
      "\n",
      "Epoch: 117\n",
      "TRAIN Loss: 0.355 | Acc: 86.500% (692/800)\n",
      "TRAIN Loss: 0.360 | Acc: 87.811% (35827/40800)\n",
      "TEST Loss: 0.438 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.610 | Acc: 81.196% (4141/5100)\n",
      "\n",
      "Epoch: 118\n",
      "TRAIN Loss: 0.330 | Acc: 88.250% (706/800)\n",
      "TRAIN Loss: 0.364 | Acc: 87.556% (35723/40800)\n",
      "TEST Loss: 0.438 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.610 | Acc: 81.255% (4144/5100)\n",
      "\n",
      "Epoch: 119\n",
      "TRAIN Loss: 0.400 | Acc: 86.875% (695/800)\n",
      "TRAIN Loss: 0.361 | Acc: 87.880% (35855/40800)\n",
      "TEST Loss: 0.440 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.611 | Acc: 81.235% (4143/5100)\n",
      "\n",
      "Epoch: 120\n",
      "TRAIN Loss: 0.372 | Acc: 86.375% (691/800)\n",
      "TRAIN Loss: 0.361 | Acc: 87.669% (35769/40800)\n",
      "TEST Loss: 0.437 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.610 | Acc: 81.157% (4139/5100)\n",
      "\n",
      "Epoch: 121\n",
      "TRAIN Loss: 0.373 | Acc: 87.750% (702/800)\n",
      "TRAIN Loss: 0.362 | Acc: 87.782% (35815/40800)\n",
      "TEST Loss: 0.439 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.612 | Acc: 81.157% (4139/5100)\n",
      "\n",
      "Epoch: 122\n",
      "TRAIN Loss: 0.430 | Acc: 84.625% (677/800)\n",
      "TRAIN Loss: 0.363 | Acc: 87.424% (35669/40800)\n",
      "TEST Loss: 0.438 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.610 | Acc: 81.216% (4142/5100)\n",
      "\n",
      "Epoch: 123\n",
      "TRAIN Loss: 0.355 | Acc: 87.750% (702/800)\n",
      "TRAIN Loss: 0.359 | Acc: 87.625% (35751/40800)\n",
      "TEST Loss: 0.438 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.612 | Acc: 81.216% (4142/5100)\n",
      "\n",
      "Epoch: 124\n",
      "TRAIN Loss: 0.385 | Acc: 86.625% (693/800)\n",
      "TRAIN Loss: 0.363 | Acc: 87.578% (35732/40800)\n",
      "TEST Loss: 0.438 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.611 | Acc: 81.353% (4149/5100)\n",
      "\n",
      "Epoch: 125\n",
      "TRAIN Loss: 0.374 | Acc: 86.875% (695/800)\n",
      "TRAIN Loss: 0.365 | Acc: 87.294% (35616/40800)\n",
      "TEST Loss: 0.435 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.608 | Acc: 81.412% (4152/5100)\n",
      "\n",
      "Epoch: 126\n",
      "TRAIN Loss: 0.343 | Acc: 88.000% (704/800)\n",
      "TRAIN Loss: 0.368 | Acc: 87.400% (35659/40800)\n",
      "TEST Loss: 0.434 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.610 | Acc: 81.216% (4142/5100)\n",
      "\n",
      "Epoch: 127\n",
      "TRAIN Loss: 0.382 | Acc: 86.750% (694/800)\n",
      "TRAIN Loss: 0.366 | Acc: 87.431% (35672/40800)\n",
      "TEST Loss: 0.436 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.609 | Acc: 81.412% (4152/5100)\n",
      "\n",
      "Epoch: 128\n",
      "TRAIN Loss: 0.338 | Acc: 87.625% (701/800)\n",
      "TRAIN Loss: 0.359 | Acc: 87.814% (35828/40800)\n",
      "TEST Loss: 0.437 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.612 | Acc: 81.196% (4141/5100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 129\n",
      "TRAIN Loss: 0.382 | Acc: 87.000% (696/800)\n",
      "TRAIN Loss: 0.360 | Acc: 87.848% (35842/40800)\n",
      "TEST Loss: 0.436 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.611 | Acc: 81.216% (4142/5100)\n",
      "\n",
      "Epoch: 130\n",
      "TRAIN Loss: 0.360 | Acc: 87.125% (697/800)\n",
      "TRAIN Loss: 0.360 | Acc: 87.652% (35762/40800)\n",
      "TEST Loss: 0.433 | Acc: 86.000% (86/100)\n",
      "TEST Loss: 0.609 | Acc: 81.431% (4153/5100)\n",
      "\n",
      "Epoch: 131\n",
      "TRAIN Loss: 0.333 | Acc: 88.875% (711/800)\n",
      "TRAIN Loss: 0.358 | Acc: 87.855% (35845/40800)\n",
      "TEST Loss: 0.433 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.612 | Acc: 81.275% (4145/5100)\n",
      "\n",
      "Epoch: 132\n",
      "TRAIN Loss: 0.383 | Acc: 86.500% (692/800)\n",
      "TRAIN Loss: 0.361 | Acc: 87.608% (35744/40800)\n",
      "TEST Loss: 0.433 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.611 | Acc: 81.314% (4147/5100)\n",
      "\n",
      "Epoch: 133\n",
      "TRAIN Loss: 0.319 | Acc: 90.375% (723/800)\n",
      "TRAIN Loss: 0.359 | Acc: 87.684% (35775/40800)\n",
      "TEST Loss: 0.435 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.611 | Acc: 81.275% (4145/5100)\n",
      "\n",
      "Epoch: 134\n",
      "TRAIN Loss: 0.369 | Acc: 88.375% (707/800)\n",
      "TRAIN Loss: 0.365 | Acc: 87.576% (35731/40800)\n",
      "TEST Loss: 0.436 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.611 | Acc: 81.373% (4150/5100)\n",
      "\n",
      "Epoch: 135\n",
      "TRAIN Loss: 0.297 | Acc: 90.875% (727/800)\n",
      "TRAIN Loss: 0.355 | Acc: 87.907% (35866/40800)\n",
      "TEST Loss: 0.436 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.612 | Acc: 81.333% (4148/5100)\n",
      "\n",
      "Epoch: 136\n",
      "TRAIN Loss: 0.355 | Acc: 86.625% (693/800)\n",
      "TRAIN Loss: 0.358 | Acc: 87.730% (35794/40800)\n",
      "TEST Loss: 0.436 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.612 | Acc: 81.412% (4152/5100)\n",
      "\n",
      "Epoch: 137\n",
      "TRAIN Loss: 0.350 | Acc: 88.250% (706/800)\n",
      "TRAIN Loss: 0.357 | Acc: 87.895% (35861/40800)\n",
      "TEST Loss: 0.433 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.611 | Acc: 81.333% (4148/5100)\n",
      "\n",
      "Epoch: 138\n",
      "TRAIN Loss: 0.375 | Acc: 87.125% (697/800)\n",
      "TRAIN Loss: 0.359 | Acc: 87.836% (35837/40800)\n",
      "TEST Loss: 0.432 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.611 | Acc: 81.255% (4144/5100)\n",
      "\n",
      "Epoch: 139\n",
      "TRAIN Loss: 0.323 | Acc: 90.375% (723/800)\n",
      "TRAIN Loss: 0.361 | Acc: 87.684% (35775/40800)\n",
      "TEST Loss: 0.435 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.612 | Acc: 81.176% (4140/5100)\n",
      "\n",
      "Epoch: 140\n",
      "TRAIN Loss: 0.372 | Acc: 86.625% (693/800)\n",
      "TRAIN Loss: 0.362 | Acc: 87.694% (35779/40800)\n",
      "TEST Loss: 0.431 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.611 | Acc: 81.294% (4146/5100)\n",
      "\n",
      "Epoch: 141\n",
      "TRAIN Loss: 0.392 | Acc: 86.125% (689/800)\n",
      "TRAIN Loss: 0.360 | Acc: 87.907% (35866/40800)\n",
      "TEST Loss: 0.437 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.611 | Acc: 81.294% (4146/5100)\n",
      "\n",
      "Epoch: 142\n",
      "TRAIN Loss: 0.364 | Acc: 87.125% (697/800)\n",
      "TRAIN Loss: 0.358 | Acc: 87.767% (35809/40800)\n",
      "TEST Loss: 0.434 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.611 | Acc: 81.333% (4148/5100)\n",
      "\n",
      "Epoch: 143\n",
      "TRAIN Loss: 0.362 | Acc: 88.875% (711/800)\n",
      "TRAIN Loss: 0.356 | Acc: 88.029% (35916/40800)\n",
      "TEST Loss: 0.436 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.612 | Acc: 81.314% (4147/5100)\n",
      "\n",
      "Epoch: 144\n",
      "TRAIN Loss: 0.351 | Acc: 87.625% (701/800)\n",
      "TRAIN Loss: 0.357 | Acc: 87.716% (35788/40800)\n",
      "TEST Loss: 0.436 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.611 | Acc: 81.255% (4144/5100)\n",
      "\n",
      "Epoch: 145\n",
      "TRAIN Loss: 0.374 | Acc: 87.625% (701/800)\n",
      "TRAIN Loss: 0.358 | Acc: 87.804% (35824/40800)\n",
      "TEST Loss: 0.436 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.611 | Acc: 81.294% (4146/5100)\n",
      "\n",
      "Epoch: 146\n",
      "TRAIN Loss: 0.358 | Acc: 88.250% (706/800)\n",
      "TRAIN Loss: 0.359 | Acc: 87.684% (35775/40800)\n",
      "TEST Loss: 0.438 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.611 | Acc: 81.314% (4147/5100)\n",
      "\n",
      "Epoch: 147\n",
      "TRAIN Loss: 0.355 | Acc: 87.125% (697/800)\n",
      "TRAIN Loss: 0.358 | Acc: 87.770% (35810/40800)\n",
      "TEST Loss: 0.436 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.611 | Acc: 81.353% (4149/5100)\n",
      "\n",
      "Epoch: 148\n",
      "TRAIN Loss: 0.365 | Acc: 87.500% (700/800)\n",
      "TRAIN Loss: 0.359 | Acc: 87.463% (35685/40800)\n",
      "TEST Loss: 0.435 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.610 | Acc: 81.255% (4144/5100)\n",
      "\n",
      "Epoch: 149\n",
      "TRAIN Loss: 0.380 | Acc: 87.250% (698/800)\n",
      "TRAIN Loss: 0.356 | Acc: 87.890% (35859/40800)\n",
      "TEST Loss: 0.438 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.612 | Acc: 81.137% (4138/5100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA8OF33Ecp4Hhwu84jtPA+P9/0VfAKuIC9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2668f2f400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "FINAL TEST LOSS: 0.5947929461729924\n",
      "CPU times: user 37min 17s, sys: 51.9 s, total: 38min 9s\n",
      "Wall time: 38min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "net = Net()\n",
    "net.cuda()\n",
    "print(net)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "train_loss_curve, test_loss_curve = [], []\n",
    "ea = 150\n",
    "\n",
    "for epoch in range(0, ea):\n",
    "    if epoch == 75 or epoch == 110:\n",
    "        learning_rate *= 0.1\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    \n",
    "plt.plot([x[0] for x in train_loss_curve], [x[1] for x in train_loss_curve], color='blue')\n",
    "plt.plot([x[0] for x in test_loss_curve], [x[1] for x in test_loss_curve], color='red')\n",
    "plt.show()\n",
    "\n",
    "net.eval()\n",
    "results = np.array([])\n",
    "for x, _ in testloader:\n",
    "    x = Variable(x).cuda()\n",
    "    y_eval = net(x)\n",
    "    if len(results)==0:\n",
    "        results = np.exp(y_eval.cpu().data.numpy())\n",
    "    else:\n",
    "        results = np.vstack((results, np.exp(y_eval.cpu().data.numpy())))\n",
    "\n",
    "net.cpu()\n",
    "\n",
    "print('\\n\\n\\nFINAL TEST LOSS: {0}'.format(log_loss(Yt, results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./dno_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./dno_2.csv') # 0.5432956804291603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (bnorm0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu): ReLU ()\n",
      "  (maxpool1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (bnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (bnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (lin1): Linear (4608 -> 384)\n",
      "  (lin2): Linear (384 -> 10)\n",
      "  (softmax): LogSoftmax ()\n",
      ")\n",
      "\n",
      "Epoch: 0\n",
      "TRAIN Loss: 2.301 | Acc: 9.375% (75/800)\n",
      "TRAIN Loss: 1.941 | Acc: 30.275% (12352/40800)\n",
      "TEST Loss: 1.464 | Acc: 49.000% (49/100)\n",
      "TEST Loss: 1.504 | Acc: 46.902% (2392/5100)\n",
      "\n",
      "Epoch: 1\n",
      "TRAIN Loss: 1.544 | Acc: 43.375% (347/800)\n",
      "TRAIN Loss: 1.505 | Acc: 45.549% (18584/40800)\n",
      "TEST Loss: 1.189 | Acc: 62.000% (62/100)\n",
      "TEST Loss: 1.264 | Acc: 56.333% (2873/5100)\n",
      "\n",
      "Epoch: 2\n",
      "TRAIN Loss: 1.365 | Acc: 53.750% (430/800)\n",
      "TRAIN Loss: 1.334 | Acc: 52.314% (21344/40800)\n",
      "TEST Loss: 1.038 | Acc: 67.000% (67/100)\n",
      "TEST Loss: 1.134 | Acc: 59.784% (3049/5100)\n",
      "\n",
      "Epoch: 3\n",
      "TRAIN Loss: 1.325 | Acc: 54.250% (434/800)\n",
      "TRAIN Loss: 1.223 | Acc: 56.593% (23090/40800)\n",
      "TEST Loss: 0.960 | Acc: 69.000% (69/100)\n",
      "TEST Loss: 1.034 | Acc: 64.196% (3274/5100)\n",
      "\n",
      "Epoch: 4\n",
      "TRAIN Loss: 1.127 | Acc: 58.750% (470/800)\n",
      "TRAIN Loss: 1.147 | Acc: 59.676% (24348/40800)\n",
      "TEST Loss: 0.910 | Acc: 68.000% (68/100)\n",
      "TEST Loss: 0.985 | Acc: 65.255% (3328/5100)\n",
      "\n",
      "Epoch: 5\n",
      "TRAIN Loss: 1.116 | Acc: 59.875% (479/800)\n",
      "TRAIN Loss: 1.092 | Acc: 61.838% (25230/40800)\n",
      "TEST Loss: 0.866 | Acc: 71.000% (71/100)\n",
      "TEST Loss: 0.943 | Acc: 67.039% (3419/5100)\n",
      "\n",
      "Epoch: 6\n",
      "TRAIN Loss: 1.064 | Acc: 62.875% (503/800)\n",
      "TRAIN Loss: 1.037 | Acc: 63.944% (26089/40800)\n",
      "TEST Loss: 0.838 | Acc: 72.000% (72/100)\n",
      "TEST Loss: 0.893 | Acc: 68.902% (3514/5100)\n",
      "\n",
      "Epoch: 7\n",
      "TRAIN Loss: 1.098 | Acc: 62.125% (497/800)\n",
      "TRAIN Loss: 0.999 | Acc: 65.125% (26571/40800)\n",
      "TEST Loss: 0.792 | Acc: 77.000% (77/100)\n",
      "TEST Loss: 0.867 | Acc: 70.196% (3580/5100)\n",
      "\n",
      "Epoch: 8\n",
      "TRAIN Loss: 0.917 | Acc: 69.125% (553/800)\n",
      "TRAIN Loss: 0.970 | Acc: 66.154% (26991/40800)\n",
      "TEST Loss: 0.784 | Acc: 75.000% (75/100)\n",
      "TEST Loss: 0.847 | Acc: 70.627% (3602/5100)\n",
      "\n",
      "Epoch: 9\n",
      "TRAIN Loss: 0.910 | Acc: 68.750% (550/800)\n",
      "TRAIN Loss: 0.929 | Acc: 67.679% (27613/40800)\n",
      "TEST Loss: 0.771 | Acc: 76.000% (76/100)\n",
      "TEST Loss: 0.825 | Acc: 71.569% (3650/5100)\n",
      "\n",
      "Epoch: 10\n",
      "TRAIN Loss: 0.878 | Acc: 68.750% (550/800)\n",
      "TRAIN Loss: 0.904 | Acc: 68.564% (27974/40800)\n",
      "TEST Loss: 0.686 | Acc: 82.000% (82/100)\n",
      "TEST Loss: 0.800 | Acc: 72.824% (3714/5100)\n",
      "\n",
      "Epoch: 11\n",
      "TRAIN Loss: 0.876 | Acc: 70.250% (562/800)\n",
      "TRAIN Loss: 0.871 | Acc: 69.610% (28401/40800)\n",
      "TEST Loss: 0.686 | Acc: 79.000% (79/100)\n",
      "TEST Loss: 0.784 | Acc: 73.078% (3727/5100)\n",
      "\n",
      "Epoch: 12\n",
      "TRAIN Loss: 0.850 | Acc: 69.375% (555/800)\n",
      "TRAIN Loss: 0.857 | Acc: 70.243% (28659/40800)\n",
      "TEST Loss: 0.727 | Acc: 78.000% (78/100)\n",
      "TEST Loss: 0.779 | Acc: 73.020% (3724/5100)\n",
      "\n",
      "Epoch: 13\n",
      "TRAIN Loss: 0.793 | Acc: 72.250% (578/800)\n",
      "TRAIN Loss: 0.835 | Acc: 70.792% (28883/40800)\n",
      "TEST Loss: 0.701 | Acc: 79.000% (79/100)\n",
      "TEST Loss: 0.763 | Acc: 73.549% (3751/5100)\n",
      "\n",
      "Epoch: 14\n",
      "TRAIN Loss: 0.817 | Acc: 70.375% (563/800)\n",
      "TRAIN Loss: 0.812 | Acc: 71.672% (29242/40800)\n",
      "TEST Loss: 0.704 | Acc: 78.000% (78/100)\n",
      "TEST Loss: 0.759 | Acc: 74.118% (3780/5100)\n",
      "\n",
      "Epoch: 15\n",
      "TRAIN Loss: 0.823 | Acc: 70.500% (564/800)\n",
      "TRAIN Loss: 0.801 | Acc: 72.120% (29425/40800)\n",
      "TEST Loss: 0.681 | Acc: 78.000% (78/100)\n",
      "TEST Loss: 0.740 | Acc: 74.569% (3803/5100)\n",
      "\n",
      "Epoch: 16\n",
      "TRAIN Loss: 0.778 | Acc: 71.875% (575/800)\n",
      "TRAIN Loss: 0.789 | Acc: 72.397% (29538/40800)\n",
      "TEST Loss: 0.637 | Acc: 81.000% (81/100)\n",
      "TEST Loss: 0.732 | Acc: 74.765% (3813/5100)\n",
      "\n",
      "Epoch: 17\n",
      "TRAIN Loss: 0.779 | Acc: 72.625% (581/800)\n",
      "TRAIN Loss: 0.770 | Acc: 73.098% (29824/40800)\n",
      "TEST Loss: 0.633 | Acc: 82.000% (82/100)\n",
      "TEST Loss: 0.722 | Acc: 75.451% (3848/5100)\n",
      "\n",
      "Epoch: 18\n",
      "TRAIN Loss: 0.747 | Acc: 71.750% (574/800)\n",
      "TRAIN Loss: 0.757 | Acc: 73.750% (30090/40800)\n",
      "TEST Loss: 0.579 | Acc: 85.000% (85/100)\n",
      "TEST Loss: 0.712 | Acc: 75.431% (3847/5100)\n",
      "\n",
      "Epoch: 19\n",
      "TRAIN Loss: 0.759 | Acc: 73.750% (590/800)\n",
      "TRAIN Loss: 0.744 | Acc: 74.346% (30333/40800)\n",
      "TEST Loss: 0.551 | Acc: 85.000% (85/100)\n",
      "TEST Loss: 0.703 | Acc: 75.765% (3864/5100)\n",
      "\n",
      "Epoch: 20\n",
      "TRAIN Loss: 0.774 | Acc: 72.250% (578/800)\n",
      "TRAIN Loss: 0.727 | Acc: 74.706% (30480/40800)\n",
      "TEST Loss: 0.588 | Acc: 82.000% (82/100)\n",
      "TEST Loss: 0.702 | Acc: 75.941% (3873/5100)\n",
      "\n",
      "Epoch: 21\n",
      "TRAIN Loss: 0.693 | Acc: 75.625% (605/800)\n",
      "TRAIN Loss: 0.716 | Acc: 75.152% (30662/40800)\n",
      "TEST Loss: 0.544 | Acc: 83.000% (83/100)\n",
      "TEST Loss: 0.705 | Acc: 75.686% (3860/5100)\n",
      "\n",
      "Epoch: 22\n",
      "TRAIN Loss: 0.777 | Acc: 73.750% (590/800)\n",
      "TRAIN Loss: 0.710 | Acc: 75.581% (30837/40800)\n",
      "TEST Loss: 0.583 | Acc: 85.000% (85/100)\n",
      "TEST Loss: 0.678 | Acc: 77.059% (3930/5100)\n",
      "\n",
      "Epoch: 23\n",
      "TRAIN Loss: 0.654 | Acc: 77.250% (618/800)\n",
      "TRAIN Loss: 0.691 | Acc: 76.294% (31128/40800)\n",
      "TEST Loss: 0.510 | Acc: 86.000% (86/100)\n",
      "TEST Loss: 0.669 | Acc: 77.451% (3950/5100)\n",
      "\n",
      "Epoch: 24\n",
      "TRAIN Loss: 0.708 | Acc: 75.375% (603/800)\n",
      "TRAIN Loss: 0.684 | Acc: 76.341% (31147/40800)\n",
      "TEST Loss: 0.500 | Acc: 86.000% (86/100)\n",
      "TEST Loss: 0.665 | Acc: 77.392% (3947/5100)\n",
      "\n",
      "Epoch: 25\n",
      "TRAIN Loss: 0.714 | Acc: 75.750% (606/800)\n",
      "TRAIN Loss: 0.673 | Acc: 76.689% (31289/40800)\n",
      "TEST Loss: 0.519 | Acc: 84.000% (84/100)\n",
      "TEST Loss: 0.654 | Acc: 77.882% (3972/5100)\n",
      "\n",
      "Epoch: 26\n",
      "TRAIN Loss: 0.671 | Acc: 75.875% (607/800)\n",
      "TRAIN Loss: 0.665 | Acc: 76.968% (31403/40800)\n",
      "TEST Loss: 0.507 | Acc: 86.000% (86/100)\n",
      "TEST Loss: 0.663 | Acc: 76.980% (3926/5100)\n",
      "\n",
      "Epoch: 27\n",
      "TRAIN Loss: 0.591 | Acc: 81.250% (650/800)\n",
      "TRAIN Loss: 0.659 | Acc: 77.081% (31449/40800)\n",
      "TEST Loss: 0.480 | Acc: 85.000% (85/100)\n",
      "TEST Loss: 0.657 | Acc: 77.961% (3976/5100)\n",
      "\n",
      "Epoch: 28\n",
      "TRAIN Loss: 0.644 | Acc: 78.000% (624/800)\n",
      "TRAIN Loss: 0.650 | Acc: 77.532% (31633/40800)\n",
      "TEST Loss: 0.494 | Acc: 84.000% (84/100)\n",
      "TEST Loss: 0.645 | Acc: 77.980% (3977/5100)\n",
      "\n",
      "Epoch: 29\n",
      "TRAIN Loss: 0.630 | Acc: 78.250% (626/800)\n",
      "TRAIN Loss: 0.644 | Acc: 77.618% (31668/40800)\n",
      "TEST Loss: 0.460 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.648 | Acc: 78.020% (3979/5100)\n",
      "\n",
      "Epoch: 30\n",
      "TRAIN Loss: 0.594 | Acc: 80.375% (643/800)\n",
      "TRAIN Loss: 0.633 | Acc: 78.167% (31892/40800)\n",
      "TEST Loss: 0.491 | Acc: 84.000% (84/100)\n",
      "TEST Loss: 0.641 | Acc: 78.059% (3981/5100)\n",
      "\n",
      "Epoch: 31\n",
      "TRAIN Loss: 0.659 | Acc: 77.000% (616/800)\n",
      "TRAIN Loss: 0.624 | Acc: 78.463% (32013/40800)\n",
      "TEST Loss: 0.469 | Acc: 86.000% (86/100)\n",
      "TEST Loss: 0.629 | Acc: 78.667% (4012/5100)\n",
      "\n",
      "Epoch: 32\n",
      "TRAIN Loss: 0.595 | Acc: 80.375% (643/800)\n",
      "TRAIN Loss: 0.618 | Acc: 78.561% (32053/40800)\n",
      "TEST Loss: 0.442 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.631 | Acc: 78.922% (4025/5100)\n",
      "\n",
      "Epoch: 33\n",
      "TRAIN Loss: 0.574 | Acc: 79.375% (635/800)\n",
      "TRAIN Loss: 0.612 | Acc: 78.821% (32159/40800)\n",
      "TEST Loss: 0.474 | Acc: 82.000% (82/100)\n",
      "TEST Loss: 0.619 | Acc: 78.765% (4017/5100)\n",
      "\n",
      "Epoch: 34\n",
      "TRAIN Loss: 0.612 | Acc: 80.000% (640/800)\n",
      "TRAIN Loss: 0.602 | Acc: 79.368% (32382/40800)\n",
      "TEST Loss: 0.454 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.620 | Acc: 79.235% (4041/5100)\n",
      "\n",
      "Epoch: 35\n",
      "TRAIN Loss: 0.648 | Acc: 76.625% (613/800)\n",
      "TRAIN Loss: 0.595 | Acc: 79.441% (32412/40800)\n",
      "TEST Loss: 0.415 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.607 | Acc: 79.373% (4048/5100)\n",
      "\n",
      "Epoch: 36\n",
      "TRAIN Loss: 0.546 | Acc: 81.250% (650/800)\n",
      "TRAIN Loss: 0.594 | Acc: 79.304% (32356/40800)\n",
      "TEST Loss: 0.408 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.607 | Acc: 79.765% (4068/5100)\n",
      "\n",
      "Epoch: 37\n",
      "TRAIN Loss: 0.566 | Acc: 80.875% (647/800)\n",
      "TRAIN Loss: 0.583 | Acc: 79.882% (32592/40800)\n",
      "TEST Loss: 0.431 | Acc: 85.000% (85/100)\n",
      "TEST Loss: 0.617 | Acc: 79.118% (4035/5100)\n",
      "\n",
      "Epoch: 38\n",
      "TRAIN Loss: 0.532 | Acc: 82.000% (656/800)\n",
      "TRAIN Loss: 0.580 | Acc: 79.733% (32531/40800)\n",
      "TEST Loss: 0.433 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.600 | Acc: 79.765% (4068/5100)\n",
      "\n",
      "Epoch: 39\n",
      "TRAIN Loss: 0.550 | Acc: 81.125% (649/800)\n",
      "TRAIN Loss: 0.567 | Acc: 80.488% (32839/40800)\n",
      "TEST Loss: 0.448 | Acc: 85.000% (85/100)\n",
      "TEST Loss: 0.606 | Acc: 79.118% (4035/5100)\n",
      "\n",
      "Epoch: 40\n",
      "TRAIN Loss: 0.616 | Acc: 79.625% (637/800)\n",
      "TRAIN Loss: 0.569 | Acc: 80.189% (32717/40800)\n",
      "TEST Loss: 0.425 | Acc: 85.000% (85/100)\n",
      "TEST Loss: 0.597 | Acc: 79.745% (4067/5100)\n",
      "\n",
      "Epoch: 41\n",
      "TRAIN Loss: 0.575 | Acc: 79.250% (634/800)\n",
      "TRAIN Loss: 0.560 | Acc: 80.681% (32918/40800)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Loss: 0.418 | Acc: 90.000% (90/100)\n",
      "TEST Loss: 0.599 | Acc: 79.765% (4068/5100)\n",
      "\n",
      "Epoch: 42\n",
      "TRAIN Loss: 0.512 | Acc: 83.375% (667/800)\n",
      "TRAIN Loss: 0.551 | Acc: 80.850% (32987/40800)\n",
      "TEST Loss: 0.410 | Acc: 86.000% (86/100)\n",
      "TEST Loss: 0.593 | Acc: 79.745% (4067/5100)\n",
      "\n",
      "Epoch: 43\n",
      "TRAIN Loss: 0.574 | Acc: 80.000% (640/800)\n",
      "TRAIN Loss: 0.551 | Acc: 80.946% (33026/40800)\n",
      "TEST Loss: 0.399 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.592 | Acc: 80.275% (4094/5100)\n",
      "\n",
      "Epoch: 44\n",
      "TRAIN Loss: 0.509 | Acc: 82.625% (661/800)\n",
      "TRAIN Loss: 0.542 | Acc: 81.451% (33232/40800)\n",
      "TEST Loss: 0.365 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.591 | Acc: 80.137% (4087/5100)\n",
      "\n",
      "Epoch: 45\n",
      "TRAIN Loss: 0.573 | Acc: 79.500% (636/800)\n",
      "TRAIN Loss: 0.541 | Acc: 81.319% (33178/40800)\n",
      "TEST Loss: 0.385 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.588 | Acc: 79.667% (4063/5100)\n",
      "\n",
      "Epoch: 46\n",
      "TRAIN Loss: 0.565 | Acc: 80.750% (646/800)\n",
      "TRAIN Loss: 0.532 | Acc: 81.672% (33322/40800)\n",
      "TEST Loss: 0.370 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.595 | Acc: 80.059% (4083/5100)\n",
      "\n",
      "Epoch: 47\n",
      "TRAIN Loss: 0.484 | Acc: 82.375% (659/800)\n",
      "TRAIN Loss: 0.526 | Acc: 81.841% (33391/40800)\n",
      "TEST Loss: 0.363 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.586 | Acc: 80.118% (4086/5100)\n",
      "\n",
      "Epoch: 48\n",
      "TRAIN Loss: 0.502 | Acc: 81.375% (651/800)\n",
      "TRAIN Loss: 0.521 | Acc: 81.934% (33429/40800)\n",
      "TEST Loss: 0.380 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.581 | Acc: 80.216% (4091/5100)\n",
      "\n",
      "Epoch: 49\n",
      "TRAIN Loss: 0.492 | Acc: 81.000% (648/800)\n",
      "TRAIN Loss: 0.511 | Acc: 82.292% (33575/40800)\n",
      "TEST Loss: 0.394 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.578 | Acc: 80.549% (4108/5100)\n",
      "\n",
      "Epoch: 50\n",
      "TRAIN Loss: 0.532 | Acc: 81.375% (651/800)\n",
      "TRAIN Loss: 0.517 | Acc: 82.032% (33469/40800)\n",
      "TEST Loss: 0.356 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.574 | Acc: 80.902% (4126/5100)\n",
      "\n",
      "Epoch: 51\n",
      "TRAIN Loss: 0.478 | Acc: 83.750% (670/800)\n",
      "TRAIN Loss: 0.507 | Acc: 82.436% (33634/40800)\n",
      "TEST Loss: 0.347 | Acc: 90.000% (90/100)\n",
      "TEST Loss: 0.581 | Acc: 80.333% (4097/5100)\n",
      "\n",
      "Epoch: 52\n",
      "TRAIN Loss: 0.566 | Acc: 79.875% (639/800)\n",
      "TRAIN Loss: 0.510 | Acc: 82.294% (33576/40800)\n",
      "TEST Loss: 0.344 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.577 | Acc: 80.667% (4114/5100)\n",
      "\n",
      "Epoch: 53\n",
      "TRAIN Loss: 0.506 | Acc: 83.250% (666/800)\n",
      "TRAIN Loss: 0.503 | Acc: 82.588% (33696/40800)\n",
      "TEST Loss: 0.309 | Acc: 91.000% (91/100)\n",
      "TEST Loss: 0.579 | Acc: 80.471% (4104/5100)\n",
      "\n",
      "Epoch: 54\n",
      "TRAIN Loss: 0.541 | Acc: 81.250% (650/800)\n",
      "TRAIN Loss: 0.497 | Acc: 82.794% (33780/40800)\n",
      "TEST Loss: 0.354 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.563 | Acc: 81.059% (4134/5100)\n",
      "\n",
      "Epoch: 55\n",
      "TRAIN Loss: 0.478 | Acc: 84.000% (672/800)\n",
      "TRAIN Loss: 0.493 | Acc: 83.017% (33871/40800)\n",
      "TEST Loss: 0.356 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.571 | Acc: 80.725% (4117/5100)\n",
      "\n",
      "Epoch: 56\n",
      "TRAIN Loss: 0.458 | Acc: 86.000% (688/800)\n",
      "TRAIN Loss: 0.489 | Acc: 82.993% (33861/40800)\n",
      "TEST Loss: 0.349 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.557 | Acc: 80.824% (4122/5100)\n",
      "\n",
      "Epoch: 57\n",
      "TRAIN Loss: 0.480 | Acc: 84.125% (673/800)\n",
      "TRAIN Loss: 0.488 | Acc: 82.917% (33830/40800)\n",
      "TEST Loss: 0.348 | Acc: 90.000% (90/100)\n",
      "TEST Loss: 0.565 | Acc: 80.980% (4130/5100)\n",
      "\n",
      "Epoch: 58\n",
      "TRAIN Loss: 0.507 | Acc: 82.125% (657/800)\n",
      "TRAIN Loss: 0.483 | Acc: 83.314% (33992/40800)\n",
      "TEST Loss: 0.363 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.569 | Acc: 80.863% (4124/5100)\n",
      "\n",
      "Epoch: 59\n",
      "TRAIN Loss: 0.436 | Acc: 85.625% (685/800)\n",
      "TRAIN Loss: 0.475 | Acc: 83.424% (34037/40800)\n",
      "TEST Loss: 0.350 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.555 | Acc: 81.235% (4143/5100)\n",
      "\n",
      "Epoch: 60\n",
      "TRAIN Loss: 0.465 | Acc: 85.125% (681/800)\n",
      "TRAIN Loss: 0.473 | Acc: 83.804% (34192/40800)\n",
      "TEST Loss: 0.371 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.561 | Acc: 81.255% (4144/5100)\n",
      "\n",
      "Epoch: 61\n",
      "TRAIN Loss: 0.473 | Acc: 84.625% (677/800)\n",
      "TRAIN Loss: 0.471 | Acc: 83.797% (34189/40800)\n",
      "TEST Loss: 0.341 | Acc: 90.000% (90/100)\n",
      "TEST Loss: 0.560 | Acc: 80.902% (4126/5100)\n",
      "\n",
      "Epoch: 62\n",
      "TRAIN Loss: 0.444 | Acc: 83.875% (671/800)\n",
      "TRAIN Loss: 0.468 | Acc: 83.853% (34212/40800)\n",
      "TEST Loss: 0.320 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.565 | Acc: 81.314% (4147/5100)\n",
      "\n",
      "Epoch: 63\n",
      "TRAIN Loss: 0.475 | Acc: 83.625% (669/800)\n",
      "TRAIN Loss: 0.465 | Acc: 83.833% (34204/40800)\n",
      "TEST Loss: 0.351 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.555 | Acc: 81.294% (4146/5100)\n",
      "\n",
      "Epoch: 64\n",
      "TRAIN Loss: 0.433 | Acc: 85.625% (685/800)\n",
      "TRAIN Loss: 0.462 | Acc: 83.907% (34234/40800)\n",
      "TEST Loss: 0.388 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.553 | Acc: 81.471% (4155/5100)\n",
      "\n",
      "Epoch: 65\n",
      "TRAIN Loss: 0.498 | Acc: 82.375% (659/800)\n",
      "TRAIN Loss: 0.459 | Acc: 83.971% (34260/40800)\n",
      "TEST Loss: 0.380 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.557 | Acc: 81.451% (4154/5100)\n",
      "\n",
      "Epoch: 66\n",
      "TRAIN Loss: 0.437 | Acc: 84.250% (674/800)\n",
      "TRAIN Loss: 0.453 | Acc: 84.289% (34390/40800)\n",
      "TEST Loss: 0.340 | Acc: 90.000% (90/100)\n",
      "TEST Loss: 0.546 | Acc: 81.627% (4163/5100)\n",
      "\n",
      "Epoch: 67\n",
      "TRAIN Loss: 0.448 | Acc: 85.000% (680/800)\n",
      "TRAIN Loss: 0.448 | Acc: 84.581% (34509/40800)\n",
      "TEST Loss: 0.328 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.544 | Acc: 81.824% (4173/5100)\n",
      "\n",
      "Epoch: 68\n",
      "TRAIN Loss: 0.430 | Acc: 84.625% (677/800)\n",
      "TRAIN Loss: 0.450 | Acc: 84.252% (34375/40800)\n",
      "TEST Loss: 0.361 | Acc: 86.000% (86/100)\n",
      "TEST Loss: 0.545 | Acc: 81.667% (4165/5100)\n",
      "\n",
      "Epoch: 69\n",
      "TRAIN Loss: 0.399 | Acc: 85.625% (685/800)\n",
      "TRAIN Loss: 0.440 | Acc: 84.664% (34543/40800)\n",
      "TEST Loss: 0.358 | Acc: 85.000% (85/100)\n",
      "TEST Loss: 0.542 | Acc: 81.882% (4176/5100)\n",
      "\n",
      "Epoch: 70\n",
      "TRAIN Loss: 0.425 | Acc: 85.500% (684/800)\n",
      "TRAIN Loss: 0.439 | Acc: 84.748% (34577/40800)\n",
      "TEST Loss: 0.349 | Acc: 90.000% (90/100)\n",
      "TEST Loss: 0.548 | Acc: 81.765% (4170/5100)\n",
      "\n",
      "Epoch: 71\n",
      "TRAIN Loss: 0.447 | Acc: 85.375% (683/800)\n",
      "TRAIN Loss: 0.434 | Acc: 85.093% (34718/40800)\n",
      "TEST Loss: 0.318 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.542 | Acc: 81.725% (4168/5100)\n",
      "\n",
      "Epoch: 72\n",
      "TRAIN Loss: 0.384 | Acc: 86.625% (693/800)\n",
      "TRAIN Loss: 0.431 | Acc: 84.907% (34642/40800)\n",
      "TEST Loss: 0.329 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.546 | Acc: 81.941% (4179/5100)\n",
      "\n",
      "Epoch: 73\n",
      "TRAIN Loss: 0.442 | Acc: 84.125% (673/800)\n",
      "TRAIN Loss: 0.430 | Acc: 84.912% (34644/40800)\n",
      "TEST Loss: 0.332 | Acc: 90.000% (90/100)\n",
      "TEST Loss: 0.541 | Acc: 81.882% (4176/5100)\n",
      "\n",
      "Epoch: 74\n",
      "TRAIN Loss: 0.375 | Acc: 88.250% (706/800)\n",
      "TRAIN Loss: 0.425 | Acc: 85.321% (34811/40800)\n",
      "TEST Loss: 0.277 | Acc: 93.000% (93/100)\n",
      "TEST Loss: 0.548 | Acc: 81.824% (4173/5100)\n",
      "\n",
      "Epoch: 75\n",
      "TRAIN Loss: 0.423 | Acc: 84.875% (679/800)\n",
      "TRAIN Loss: 0.425 | Acc: 85.125% (34731/40800)\n",
      "TEST Loss: 0.329 | Acc: 88.000% (88/100)\n",
      "TEST Loss: 0.550 | Acc: 81.549% (4159/5100)\n",
      "\n",
      "Epoch: 76\n",
      "TRAIN Loss: 0.393 | Acc: 86.125% (689/800)\n",
      "TRAIN Loss: 0.420 | Acc: 85.400% (34843/40800)\n",
      "TEST Loss: 0.340 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.548 | Acc: 81.725% (4168/5100)\n",
      "\n",
      "Epoch: 77\n",
      "TRAIN Loss: 0.406 | Acc: 85.250% (682/800)\n",
      "TRAIN Loss: 0.417 | Acc: 85.515% (34890/40800)\n",
      "TEST Loss: 0.330 | Acc: 87.000% (87/100)\n",
      "TEST Loss: 0.540 | Acc: 82.314% (4198/5100)\n",
      "\n",
      "Epoch: 78\n",
      "TRAIN Loss: 0.398 | Acc: 86.625% (693/800)\n",
      "TRAIN Loss: 0.416 | Acc: 85.755% (34988/40800)\n",
      "TEST Loss: 0.328 | Acc: 89.000% (89/100)\n",
      "TEST Loss: 0.540 | Acc: 81.922% (4178/5100)\n",
      "\n",
      "Epoch: 79\n",
      "TRAIN Loss: 0.413 | Acc: 85.500% (684/800)\n",
      "TRAIN Loss: 0.411 | Acc: 85.637% (34940/40800)\n",
      "TEST Loss: 0.405 | Acc: 83.000% (83/100)\n",
      "TEST Loss: 0.547 | Acc: 81.667% (4165/5100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAr8xxhQYC/zGGFNgLPAbY0yB+X9Okq4AJhat2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3881520a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "FINAL TEST LOSS: 0.5493827520081097\n",
      "CPU times: user 20min 23s, sys: 42.9 s, total: 21min 6s\n",
      "Wall time: 21min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "net = Net()\n",
    "net.cuda()\n",
    "print(net)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "train_loss_curve, test_loss_curve = [], []\n",
    "ea = 80\n",
    "\n",
    "for epoch in range(0, ea):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    \n",
    "plt.plot(train_loss_curve, color='blue')\n",
    "plt.plot(test_loss_curve, color='red')\n",
    "plt.show()\n",
    "\n",
    "net.eval()\n",
    "results = np.array([])\n",
    "for x, _ in testloader:\n",
    "    x = Variable(x).cuda()\n",
    "    y_eval = net(x)\n",
    "    if len(results)==0:\n",
    "        results = np.exp(y_eval.cpu().data.numpy())\n",
    "    else:\n",
    "        results = np.vstack((results, np.exp(y_eval.cpu().data.numpy())))\n",
    "\n",
    "net.cpu()\n",
    "\n",
    "print('\\n\\n\\nFINAL TEST LOSS: {0}'.format(log_loss(Yt, results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./dno_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO \n",
    "# - дропаут\n",
    "# - пинок градиента\n",
    "# - аугментация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.cuda()\n",
    "print(net)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-4\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "loss_curve = []\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "net = Net()\n",
    "net.cuda()\n",
    "learning_rate = 1e-4\n",
    "batch_size = 3\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "loss_curve = []\n",
    "\n",
    "for iter in range(3):\n",
    "    Xperm = np.random.permutation(X.shape[0])\n",
    "    loss_acc = 0\n",
    "    for b in range(X.shape[0]//batch_size):\n",
    "        batch_idxs = Xperm[b*batch_size:(b+1)*batch_size]\n",
    "        x = Variable(torch.Tensor(X[batch_idxs])).cuda()\n",
    "        y = Variable(torch.LongTensor(Y[batch_idxs])).cuda()\n",
    "        y_pred = net(x)\n",
    "              \n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        loss_acc += loss.data[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Done epoch %s with loss %s' % (iter, loss_acc))\n",
    "    loss_curve.append(loss_acc)\n",
    "    \n",
    "plt.plot(loss_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
